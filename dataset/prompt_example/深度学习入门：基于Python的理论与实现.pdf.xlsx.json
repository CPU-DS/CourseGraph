[
    {
        "input": "后面我们会介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weightdecay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习的方法。通过减小权重参数的值来抑制过拟合的发生。\n如果想减小权重的值，一开始就将初始值设为较小的值才是正途。实际上，在这之前的权重初始值都是像0.01 * np.random.randn(10, 100)这样，使用由高斯分布生成的值乘以0.01后得到的值（标准差为0.01的高斯分布）。\n如果我们把权重初始值全部设为0以减小权重的值，会怎么样呢？从结论来说，将权重初始值设为0不是一个好主意。事实上，将权重初始值设为0的话，将无法正确进行学习。\n为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新（回忆一下“乘法节点的反向传播”\n的内容）。因此，权重被更新为相同的值，并拥有了对称的值（重复的值）这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。",
        "entities": [
            "过拟合",
            "权值衰减",
            "权重均一化"
        ],
        "output": "这段文字提到了抑制过拟合、提高泛化能力的技巧——权值衰减，以及为什么不能将权重全部初始化为相同的值（防止权重均一化）。返回为```json\n[\"过拟合\", \"权值衰减\", \"权重均一化\"]\n```"
    },
    {
        "input": "本书提供了im2col函数，并将这个im2col函数作为黑盒（不关心内部实使用。im2col的实现内容在common/util.py中，它的实现（实质上）是一个行左右的简单函数。有兴趣的读者可以参考。\nim2col这一便捷函数具有以下接口。\nim2col (input_data, filter_h, filter_w, stride=1, pad=0)• input_data―由（数据量，通道，高，长）的4维数组构成的输入数据• filter_h―滤波器的高• filter_w―滤波器的长• stride―步幅• pad―填充\nim2col会考虑滤波器大小、步幅、填充，将输入数据展开为2维数组。\n现在，我们来实际使用一下这个im2col。\nimport sys, ossys.path.append(os.pardir)from common.util import im2colx1 = np.random.rand(1, 3, 7, 7)col1 = im2col(x1, 5, 5, stride=1, pad=0)print(col1.shape) # (9, 75)x2 = np.random.rand(10, 3, 7, 7) # 10个数据col2 = im2col(x2, 5, 5, stride=1, pad=0)print(col2.shape) # (90, 75)这里举了两个例子。第一个是批大小为1、通道为3的7 × 7的数据，第二个的批大小为10，数据形状和第一个相同。分别对其应用im2col函数，在这两种情形下，第2维的元素个数均为75。这是滤波器（通道为3、大小为5 × 5）的元素个数的总和。批大小为1时，im2col的结果是(9, 75)。而第2个例子中批大小为10，所以保存了10倍的数据，即(90, 75)。\n现在使用im2col来实现卷积层。这里我们将卷积层实现为名为Convolution的类。\nclass Convolution:    def __init__(self, W, b, stride=1, pad=0):        self.W = W        self.b = b        self.stride = stride        self.pad = pad    def forward(self, x):        FN, C, FH, FW = self.W.shape        N, C, H, W = x.shape        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)        col = im2col(x, FH, FW, self.stride, self.pad)        col_W = self.W.reshape(FN, -1).T # 滤波器的展开        out = np.dot(col, col_W) + self.b        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)        return out\n卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。\n滤波器是(FN, C, FH, FW) 的4 维形状。另外，FN、C、FH、FW 分别是FilterNumber（滤波器数量）、Channel、Filter Height、Filter Width的缩写。\n这里用粗体字表示Convolution层的实现中的重要部分。在这些粗体字部分，用im2col展开输入数据，并用reshape将滤波器展开为2维数组。然后，计算展开后的矩阵的乘积。\n展开滤波器的部分（代码段中的粗体字）如图7-19所示，将各个滤波器的方块纵向展开为1 列。这里通过reshape(FN,-1)将参数指定为-1，这是reshape的一个便利的功能。通过在reshape时指定为-1，reshape函数会自动计算-1维度上的元素个数，以使多维数组的元素个数前后一致。比如，(10, 3, 5, 5)形状的数组的元素个数共有750个，指定reshape(10,-1)后，就会转换成(10, 75)形状的数组。\nforward的实现中，最后会将输出大小转换为合适的形状。转换时使用了NumPy的transpose函数。transpose会更改多维数组的轴的顺序。如图7-20所示，通过指定从0开始的索引（编号）序列，就可以更改轴的顺序。\n以上就是卷积层的forward处理的实现。通过使用im2col进行展开，基本上可以像实现全连接层的Affine层一样来实现（5.6节）。接下来是卷积层的反向传播的实现，因为和Affine层的实现有很多共通的地方，所以就不再介绍了。但有一点需要注意，在进行卷积层的反向传播时，必须进行im2col的逆处理。这可以使用本书提供的col2im函数（col2im的实现在common/util.\ny中）来进行。除了使用col2im这一点，卷积层的反向传播和Affi  ne层的实现方式都一样。卷积层的反向传播的实现在common/layer.py中，有兴趣的读者可以参考。",
        "entities": [],
        "output": "这段文字介绍了im2col函数的具体用法，但是没有介绍一个新的概念或者引入新的名词，所以没有能够抽取出来的知识点。返回为```json\n[]\n```"
    },
    {
        "input": "现在我们试着用上述的数值微分对简单函数进行求导。先来看一个由下式表示的2次函数。\n>>> numerical_diff(function_1, 5)0.1999999999990898>>> numerical_diff(function_1, 10)0.2999999999986347\n这里计算的导数是f(x) 相对于x 的变化量，对应函数的斜率。另外，f(x) = 0.01x2 + 0.1x 的解析解是。因此，在x = 5 和x = 10处，“真的导数”分别为0.2和0.3。和上面的结果相比，我们发现虽然严格意义上它们并不一致，但误差非常小。实际上，误差小到基本上可以认为它们是相等的。\n现在，我们用上面的数值微分的值作为斜率，画一条直线。结果如图4-7所示，可以确认这些直线确实对应函数的切线（源代码在ch04/gradient_1d.\npy中）。",
        "entities": [],
        "output": "这段文字可能在介绍使用数值微分进行求导，但是没有介绍一个新的概念或者引入新的名词，所以没有能够抽取出来的知识点。返回为```json\n[]\n```"
    },
    {
        "input": "神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算（NumPy 中是np.dot()，具体请参照3.3 节）。比如，还记得我们用Python进行了下面的实现吗？\n>>> X = np.random.rand(2)   # 输入>>> W = np.random.rand(2,3) # 权重>>> B = np.random.rand(3)   # 偏置>>>>>> X.shape # (2,)>>> W.shape # (2, 3)>>> B.shape # (3,)>>>>>> Y = np.dot(X, W) + B这里，X、W、B 分别是形状为(2,)、(2, 3)、(3,)的多维数组。这样一来，神经元的加权和可以用Y = np.dot(X, W) + B计算出来。然后，Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。此外，我们来复习一下，矩阵的乘积运算的要点是使对应维度的元素个数一致。比如，如下面的图5-23所示，X和W 的乘积必须使对应维度的元素个数一致。\n另外，这里矩阵的形状用(2, 3)这样的括号表示（为了和NumPy的shape属性的输出一致）。\n>>> X = np.random.rand(2)   # 输入>>> W = np.random.rand(2,3) # 权重>>> B = np.random.rand(3)   # 偏置>>>>>> X.shape # (2,)>>> W.shape # (2, 3)>>> B.shape # (3,)>>>>>> Y = np.dot(X, W) + B\n神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”A。因此，这里将进行仿射变换的处理实现为“Affine层”。\n现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。\n将乘积运算用“dot”节点表示的话，则np.dot(X, W) + B的运算可用图5-24所示的计算图表示出来。另外，在各个变量的上方标记了它们的形状（比如，计算图上显示了X的形状为(2,)，X·W的形状为(3,)等）。\n图5-24是比较简单的计算图，不过要注意X、W、B是矩阵（多维数组）。\n之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。\n现在我们来考虑图5-24的计算图的反向传播。以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。实际写一下的话，可以得到下式（这里省略了式（5.13）的推导过程）。\n（5.13）\n式（5.13）中W T的T表示转置。转置操作会把W的元素(i, j)换成元素j, i)。用数学式表示的话，可以写成下面这样。\n（5.14）\n如式（5.14）所示，如果W的形状是(2, 3)，W T的形状就是(3, 2)。\n现在，我们根据式（5.13），尝试写出计算图的反向传播，如图5-25所示。\n我们看一下图2-25的计算图中各个变量的形状。尤其要注意，X和形状相同，W和形状相同。从下面的数学式可以很明确地看出X和形状相同。\n（5.15）\n为什么要注意矩阵的形状呢？因为矩阵的乘积运算要求对应维度的元素个数保持一致，通过确认一致性，就可以导出式（5.13）。比如，的形状是(3,)，W的形状是(2, 3)时，思考和W T的乘积，使得的形状为(2,)（图5-26）。这样一来，就会自然而然地推导出式（5.13）。",
        "entities": [
            "正向传播",
            "仿射变换",
            "Affine层"
        ],
        "output": "这段文字介绍了神经网络中的正向传播流程，以及其中的矩阵乘法在几何学领域被称为仿射变换，通过Affine层实现。返回为```json\n[\"正向传播\", \"仿射变换\", \"Affine层\"]\n```"
    },
    {
        "input": "异或门也被称为逻辑异或电路。如图2-5所示，仅当x1或x2中的一方为1时，才会输出1（“异或”是拒绝其他的意思）。那么，要用感知机实现这个异或门的话，应该设定什么样的权重参数呢？\n实际上，用前面介绍的感知机是无法实现这个异或门的。为什么用感知机可以实现与门、或门，却无法实现异或门呢？下面我们尝试通过画图来思考其中的原因。\n首先，我们试着将或门的动作形象化。或门的情况下，当权重参数(b, w1, w2) = (−0.5, 1.0, 1.0)时，可满足图2-4的真值表条件。此时，感知机可用下面的式（2.3）表示。\n（2.3）\n式（2.3）表示的感知机会生成由直线−0.5 + x1 + x2 = 0分割开的两个空间。其中一个空间输出1，另一个空间输出0，如图2-6所示。\n或门在(x1, x2) = (0, 0) 时输出0，在(x1, x2) 为(0, 1)、(1, 0)、(1, 1) 时输出1。图2-6中，○表示0，△表示1。如果想制作或门，需要用直线将图2-\n中的○和△分开。实际上，刚才的那条直线就将这4个点正确地分开了。\n那么，换成异或门的话会如何呢？能否像或门那样，用一条直线作出分割图2-7中的○和△的空间呢？\n想要用一条直线将图2-7中的○和△分开，无论如何都做不到。事实上，用一条直线是无法将○和△分开的。",
        "entities": [
            "异或门"
        ],
        "output": "这段文字介绍了异或门的定义以及通过感知机来实现异或门。返回为```json\n[\"异或门\"]\n```"
    },
    {
        "input": "虽然通过GPU可以实现深度学习运算的高速化，但即便如此，当网络较深时，学习还是需要几天到几周的时间。并且，前面也说过，深度学习伴随着很多试错。为了创建良好的网络，需要反复进行各种尝试，这样一来就必然会产生尽可能地缩短一次学习所需的时间的要求。于是，将深度学习的学习过程扩展开来的想法（也就是分布式学习）就变得重要起来。\n为了进一步提高深度学习所需的计算的速度，可以考虑在多个GPU或\n者多台机器上进行分布式计算。现在的深度学习框架中，出现了好几个支持多GPU或者多机器的分布式学习的框架。其中，Google的TensorFlow、微软的CNTK（Computational Network Toolki）在开发过程中高度重视分布式学习。以大型数据中心的低延迟·高吞吐网络作为支撑，基于这些框架的分布式学习呈现出惊人的效果。\n基于分布式学习，可以达到何种程度的高速化呢？图8-16中显示了基于TensorFlow的分布式学习的效果。\n如图8-16所示，随着GPU个数的增加，学习速度也在提高。实际上，与使用1 个GPU 时相比，使用100 个GPU（设置在多台机器上，共100 个）似乎可以实现56倍的高速化！这意味着之前花费7天的学习只要3个小时就能完成，充分说明了分布式学习惊人的效果。\n关于分布式学习，“如何进行分布式计算”是一个非常难的课题。它包含了机器间的通信、数据的同步等多个无法轻易解决的问题。可以将这些难题都交给TensorFlow等优秀的框架。这里，我们不讨论分布式学习的细节。\n关于分布式学习的技术性内容，请参考TensorFlow的技术论文（白皮书）等。",
        "entities": [
            "分布式学习"
        ],
        "output": "这段文字介绍了为了缩短训练时间，可以在多个GPU或者多台机器上进行分布式学习。返回为```json\n[\"分布式学习\"]\n```"
    },
    {
        "input": "图像分割是指在像素水平上对图像进行分类。如图8-19所示，使用以像素为单位对各个对象分别着色的监督数据进行学习。然后，在推理时，对输入图像的所有像素进行分类。\n之前实现的神经网络是对图像整体进行了分类，要将它落实到像素水平的话，该怎么做呢？\n要基于神经网络进行图像分割，最简单的方法是以所有像素为对象，对每个像素执行推理处理。比如，准备一个对某个矩形区域中心的像素进行分类的网络，以所有像素为对象执行推理处理。正如大家能想到的，这样的方法需要按照像素数量进行相应次forward处理，因而需要耗费大量的时间（正确地说，卷积运算中会发生重复计算很多区域的无意义的计算）。为了解决这个无意义的计算问题，有人提出了一个名为FCN（Fully ConvolutionalNetwork）[37]的方法。该方法通过一次forward处理，对所有像素进行分类（图8-20）。\nFCN的字面意思是 “全部由卷积层构成的网络”。相对于一般的CNN包含全连接层，FCN将全连接层替换成发挥相同作用的卷积层。在物体识别中使用的网络的全连接层中，中间数据的空间容量被作为排成一列的节点进\n行处理，而只由卷积层构成的网络中，空间容量可以保持原样直到最后的输出。\n如图8-20所示，FCN的特征在于最后导入了扩大空间大小的处理。基于这个处理，变小了的中间数据可以一下子扩大到和输入图像一样的大小。\nFCN最后进行的扩大处理是基于双线性插值法的扩大（双线性插值扩大）。\nFCN中，这个双线性插值扩大是通过去卷积（逆卷积运算）来实现的（细节请参考FCN的论文[37]）。\n全连接层中，输出和全部的输入相连。使用卷积层也可以实现与此结构完全相同的连接。比如，针对输入大小是32×10×10（通道数32、高10、长10）的数据的全连接层可以替换成滤波器大小为32×10×10的卷积层。如果全连接层的输出节点数是100，那么在卷积层准备100个32×10×10的滤波器就可以实现完全相同的处理。\n像这样，全连接层可以替换成进行相同处理的卷积层。",
        "entities": [
            "图像分割",
            "FCN"
        ],
        "output": "这段文字介绍了图像的分割的定义，以及通过FCN的神经网络方法进行图像分割。返回为```json\n[\"图像分割\", \"FCN\"]\n```"
    },
    {
        "input": "在一个标题为“What is the class of this image ?”的网站[32]上，以排行榜的形式刊登了目前为止通过论文等渠道发表的针对各种数据集的方法的识别精度（图8-3）。\n观察图8-3 的排行结果，可以发现“Neural Networks” “Deep” “Convolutional”等关键词特别显眼。实际上，排行榜上的前几名大都是基于CNN的方法。顺便说一下，截止到2016年6月，对MNIST数据集的最高识别精度是99.79%（错误识别率为0.21%），该方法也是以CNN为基础的[33]。\n不过，它用的CNN并不是特别深层的网络（卷积层为2层、全连接层为2层的网络）。\n对于MNIST数据集，层不用特别深就获得了（目前）最高的识别精度。一般认为，这是因为对于手写数字识别这样一个比较简单的任务，没有必要将网络的表现力提高到那么高的程度。因此，可以说加深层的好处并不大。而之后要介绍的大规模的一般物体识别的情况，因为问题复杂，所以加深层对提高识别精度大有裨益。\n参考刚才排行榜中前几名的方法，可以发现进一步提高识别精度的技术和线索。比如，集成学习、学习率衰减、Data Augmentation（数据扩充）等都有助于提高识别精度。尤其是Data Augmentation，虽然方法很简单，但在提高识别精度上效果显著。\nData Augmentation基于算法“人为地”扩充输入图像（训练图像）。具体地说，如图8-4所示，对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。这在数据集的图像数量有限时尤其有效。\n除了如图8-4所示的变形之外，Data Augmentation还可以通过其他各种方法扩充图像，比如裁剪图像的 “crop处理”、将图像左右翻转的“fl ip处理”A 等。对于一般的图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。不管怎样，通过Data Augmentation巧妙地增加训练图像，就可以提高深度学习的识别精度。虽然这个看上去只是一个简单的技巧，不过经常会有很好的效果。这里，我们不进行Data Augmentation的实现，不\n过这个技巧的实现比较简单，有兴趣的读者请自己试一下。",
        "entities": [
            "集成学习",
            "学习率衰减",
            "Data Augmentation"
        ],
        "output": "这段文字介绍了几种进一步提高识别精度的技术和线索，包括集成学习、学习率衰减等，特别是Data Augmentation（数据扩充），在数据集的图像数量有限时尤其有效。返回为```json\n[\"集成学习\", \"学习率衰减\", \"Data Augmentation\"]\n```"
    },
    {
        "input": "感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流/不流”（1/0）两种取值。在本书中，0对应“不传递信号”，1对应“传递信号”。\n图2-1是一个接收两个输入信号的感知机的例子。x1、x2是输入信号，y 是输出信号，w1、w2 是权重（w 是weight 的首字母）。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重\nw1x1、w2x2）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。\n（2.1）\n感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。\n权重相当于电流里所说的电阻。电阻是决定电流流动难度的参数，电阻越低，通过的电流就越大。而感知机的权重则是值越大，通过的信号就越大。不管是电阻还是权重，在控制信号流动难度（或者流动容易度）这一点上的作用都是一样的。",
        "entities": [
            "感知机",
            "权重"
        ],
        "output": "这段文字介绍了感知机的具象化定义以及一种的权重的作用。返回为```json\n[\"感知机\", \"权重\"]\n```"
    },
    {
        "input": "下面，我们来介绍矩阵（二维数组）的乘积。比如2 × 2的矩阵，其乘积可以像图3-11这样进行计算（按图中顺序进行计算是规定好了的）。\n如本例所示，矩阵的乘积是通过左边矩阵的行（横向）和右边矩阵的列（纵向）以对应元素的方式相乘后再求和而得到的。并且，运算的结果保存为新的多维数组的元素。比如，A的第1行和B的第1列的乘积结果是新数组的第1行第1列的元素，A的第2行和B的第1列的结果是新数组的第2行第1列的元素。另外，在本书的数学标记中，矩阵将用黑斜体表示（比如，矩阵A），以区别于单个元素的标量（比如，a或b）。这个运算在Python中可以用如下代码实现。\n>>> A = np.array([[1,2], [3,4]])\n>>> A.shape(2, 2)>>> B = np.array([[5,6], [7,8]])>>> B.shape(2, 2)>>> np.dot(A, B)array([[19, 22],       [43, 50]])这里，A 和B 都是2 × 2 的矩阵，它们的乘积可以通过NumPy 的np.dot()函数计算（乘积也称为点积）。np.dot()接收两个NumPy数组作为参数，并返回数组的乘积。这里要注意的是，np.dot(A, B)和np.dot(B, A)的值可能不一样。和一般的运算（+或*等）不同，矩阵的乘积运算中，操作数（A、B）的顺序不同，结果也会不同。\n这里介绍的是计算2 × 2形状的矩阵的乘积的例子，其他形状的矩阵的乘积也可以用相同的方法来计算。比如，2 × 3的矩阵和3 × 2 的矩阵的乘积可按如下形式用Python来实现。\n>>> A = np.array([[1,2,3], [4,5,6]])>>> A.shape(2, 3)>>> B = np.array([[1,2], [3,4], [5,6]])>>> B.shape(3, 2)>>> np.dot(A, B)array([[22, 28],       [49, 64]])2 × 3的矩阵A和3 × 2的矩阵B的乘积可按以上方式实现。这里需要注意的是矩阵的形状（shape）。具体地讲，矩阵A的第1维的元素个数（列数）必须和矩阵B的第0维的元素个数（行数）相等。在上面的例子中，矩阵A的形状是2 × 3，矩阵B的形状是3 × 2，矩阵A的第1维的元素个数（3）和矩阵B的第0维的元素个数（3）相等。如果这两个值不相等，则无法计算矩阵的乘积。比如，如果用Python计算2 × 3 的矩阵A和2 × 2的矩阵C的乘积，则会输出如下错误。\n>>> C = np.array([[1,2], [3,4]])>>> C.shape\n(2, 2)>>> A.shape(2, 3)>>> np.dot(A, C)Traceback (most recent call last):  File \"<stdin>\", line 1, in <module>ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)这个错误的意思是，矩阵A的第1维和矩阵C的第0维的元素个数不一致（维度的索引从0开始）。也就是说，在多维数组的乘积运算中，必须使两个矩阵中的对应维度的元素个数一致，这一点很重要。我们通过图3-12再来确认一下。\n这个错误的意思是，矩阵A的第1维和矩阵C的第0维的元素个数不一致（维度的索引从0开始）。也就是说，在多维数组的乘积运算中，必须使两个矩阵中的对应维度的元素个数一致，这一点很重要。我们通过图3-12再来确认一下。\n图3-12中，3 × 2的矩阵A和2 × 4 的矩阵B的乘积运算生成了3 × 4的矩阵C。如图所示，矩阵A和矩阵B的对应维度的元素个数必须保持一致。\n此外，还有一点很重要，就是运算结果的矩阵C的形状是由矩阵A的行数和矩阵B的列数构成的。\n另外，当A是二维矩阵、B是一维数组时，如图3-13所示，对应维度的元素个数要保持一致的原则依然成立。\n可按如下方式用Python实现图3-13的例子。",
        "entities": [
            "矩阵乘法",
            "np.dot()"
        ],
        "output": "这段文字介绍了矩阵乘法的运算规则以及可以通过Numpy的np.dot()函数计算点积。返回为```json\n[\"矩阵乘法\", \"np.dot()\"]\n```"
    },
    {
        "input": "阶跃函数和sigmoid函数还有其他共同点，就是两者均为非线性函数。\nsigmoid函数是一条曲线，阶跃函数是一条像阶梯一样的折线，两者都属于非线性的函数。\n在介绍激活函数时，经常会看到“非线性函数”和“线性函数”等术语。\n函数本来是输入某个值后会返回一个值的转换器。向这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学式表示为h(x) = cx。c为常数）。因此，线性函数是一条笔直的直线。\n而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。\n神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。\n线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。为了具体地（稍微直观地）理解这一点，我们来思考下面这个简单的例子。这里我们考虑把线性函数h(x) = cx 作为激活函数，把y(x) = h(h(h(x))) 的运算对应3 层神经网络A。这个运算会进行y(x) = c × c × c × x的乘法运算，但是同样的处理可以由y(x) = ax（注意，a = c 3）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示，使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。",
        "entities": [
            "阶跃函数",
            "sigmod函数",
            "线性函数",
            "非线性函数"
        ],
        "output": "这段文字介绍了阶跃函数和sigmoid函数都属于非线性函数，以及激活函数只能使用非线性函数，不能使用线性函数。返回为```json\n[\"阶跃函数\", \"sigmod函数\", \"线性函数\", \"非线性函数\"]\n```"
    },
    {
        "input": "NumPy不仅可以生成一维数组（排成一列的数组），也可以生成多维数组比如，可以生成如下的二维数组（矩阵）。\n比如，可以生成如下的二维数组（矩阵）。\n>>> A = np.array([[1, 2], [3, 4]])>>> print(A)[[1 2] [3 4]]>>> A.shape(2, 2)>>> A.dtypedtype('int64')这里生成了一个2 × 2的矩阵A。另外，矩阵A的形状可以通过shape查看，矩阵元素的数据类型可以通过dtype查看。\n下面，我们来看一下矩阵的算术运算。\n>>> B = np.array([[3, 0],[0, 6]])>>> A + B\n>>> B = np.array([[3, 0],[0, 6]])>>> A + Barray([[ 4,  2],       [ 3, 10]])>>> A * Barray([[ 3,  0],       [ 0, 24]])和数组的算术运算一样，矩阵的算术运算也可以在相同形状的矩阵间以对应元素的方式进行。并且，也可以通过标量（单一数值）对矩阵进行算术运算。\n这也是基于广播的功能。\n和数组的算术运算一样，矩阵的算术运算也可以在相同形状的矩阵间以对应元素的方式进行。并且，也可以通过标量（单一数值）对矩阵进行算术运算这也是基于广播的功能。\nNumPy数组（np.array）可以生成N维数组，即可以生成一维数组、二维数组、三维数组等任意维数的数组。数学上将一维数组称为向量，将二维数组称为矩阵。另外，可以将一般化之后的向量或矩阵等统称为张量（tensor）。本书基本上将二维数组称为“矩阵”，将三维数组及三维以上的数组称为“张量”或“多维数组”。",
        "entities": [
            "Numpy",
            "多维数组",
            "矩阵",
            "张量",
            "广播"
        ],
        "output": "这段文字介绍了Numpy可以生成多维数组，并且数组之间可以通过广播功能进行运算。返回为```json\n[\"Numpy\", \"多维数组\", \"矩阵\", \"张量\", \"广播\"]\n```"
    },
    {
        "input": "除了单一的数值，还可以用列表（数组）汇总数据。\n>>> a = [1, 2, 3, 4, 5] # 生成列表>>> print(a)  # 输出列表的内容[1, 2, 3, 4, 5]>>> len(a)    # 获取列表的长度5>>> a[0]      # 访问第一个元素的值1>>> a[4]5>>> a[4] = 99 # 赋值>>> print(a)[1, 2, 3, 4, 99]\n元素的访问是通过a[0]这样的方式进行的。[]中的数字称为索引（下标），索引从0 开始（索引0 对应第一个元素）。此外，Python 的列表提供了切片（slicing）这一便捷的标记法。使用切片不仅可以访问某个值，还可以访问列表的子列表（部分列表）。\n>>> print(a)[1, 2, 3, 4, 99]>>> a[0:2] # 获取索引为0到2（不包括2！\n）的元素[1, 2]>>> a[1:]  # 获取从索引为1的元素到最后一个元素[2, 3, 4, 99]\n>>> a[:3]  # 获取从第一个元素到索引为3（不包括3！\n）的元素[1, 2, 3]>>> a[:-1] # 获取从第一个元素到最后一个元素的前一个元素之间的元素[1, 2, 3, 4]>>> a[:-2] # 获取从第一个元素到最后一个元素的前二个元素之间的元素[1, 2, 3]\n进行列表的切片时，需要写成a[0:2]这样的形式。a[0:2]用于取出从索引为0的元素到索引为2的元素的前一个元素之间的元素。另外，索引−1对应最后一个元素，−2对应最后一个元素的前一个元素。",
        "entities": [],
        "output": "这段文字在介绍python的列表功能，没有能够抽取出来的知识点。返回为```json\n[]\n```"
    },
    {
        "input": "可以使用matplotlib的pyplot模块绘制图形。话不多说，我们来看一个绘制sin函数曲线的例子。\nimport numpy as npimport matplotlib.pyplot as plt\nplt.plot(x, y)plt.show()这里使用NumPy的arange方法生成了[0, 0.1, 0.2, ���, 5.8, 5.9]的数据，将其设为x。对x的各个元素，应用NumPy的sin函数np.sin()，将x、y的数据传给plt.plot方法，然后绘制图形。最后，通过plt.show()显示图形。\n运行上述代码后，就会显示图1-3所示的图形。",
        "entities": [],
        "output": "这段文字在介绍使用pyplot绘制sin函数，没有能够抽取出来的知识点。返回为```json\n[]\n```"
    }
]
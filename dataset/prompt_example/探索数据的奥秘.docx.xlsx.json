[
    {
        "input": "6.2.2 线性回归模型性能评价\n例6-2-1（a）演示了如何利用sklearn库中的现成工具LinearRegression来实现线性回归。不知大家是否注意到，我们在用其中的fit函数时，其实除了要求自变量x和因变量y等长，以及该长度（即训练集样本容量）大于1，再没有任何其他限制。也就是说，只要有等长的且序列长度超过1的x和y，就一定能找到一个回归模型。但是，这个模型到底是好还是不好呢？或者，从数学的角度来说，只要我们能给出一定量的样本，利用最小二乘法总是能找到一条回归线（例如图6-7的左、右两幅子图中，都找到了绿色实线代表的回归直线）。但是，这条回归线只代表了所有同阶次线中与样本间RMSE最小的一条，并不代表这个RMSE就是一个可接受的RMSE。例如例6-2-1（a）中，没有一个比较标准，很难确定约0.40的RMSE是好还是不好的。\n图6-2-2 回归模型总是能找到\n那么，怎么评价我们的回归模型好与不好呢？或者说：我们真地能有效利用这条回归线去通过x预测y吗；更直白地，得到的回归有意义吗？我们必须引入一个回归效果的评价参数，即决定系数。\n我们可借助图6-2-3来做几个定义。\n图6-2-3 的意义\n对所有的样本，定义总平方和为：\n     （6-2-8）\n它代表样本相对于样本均值的总离差平方和。结合式（6-2-5）定义的残差平方和，可给出的定义，即\n    （6-2-9）\n进一步还可定义回归平方和为：\n  （6-2-10）\n它代表由于回归函数所引入的样本相对于样本均值的离差，属于样本变异性中可以被回归模型解释的部分。数学上可证明，最小二乘法确定的回归模型中，\n    （6-2-11）\n因此也可以由：\n    （6-2-12）\n求得。根据以上定义，衡量的是数据总变异性（总平方和）中可由模型解释的变异性（回归平方和）所占的比例，其取值在0到1之间，越靠近1，则数据中可由模型解释的成分越多，从而代表模型性能越好；越接近0，则数据中的模型不可解释成分越多，模型性能越不好。\n例6-2-1（b） 线性回归评价举例。\nprint('r_square = ',linreg.score(x,y))\nLinearRegression.score方法能直接返回。我们看到6-2-1（a）中回归模型约0.76，可见并不是特别好。",
        "entities": [
            "LinearRegression",
            "线性回归",
            "RMSE",
            "决定系数"
        ],
        "output": "这段文字介绍了使用LinearRegression来实现线性回归，但这条回归线只代表了所有同阶次线中与样本间RMSE最小的一条，所以使用决定系数评价回归效果。返回为```json\n[\"LinearRegression\", \"线性回归\", \"RMSE\", \"决定系数\"]\n```"
    },
    {
        "input": "5.1.2特征的数据类型及意义\n数据的每一列（特征）在文件中的存储数据类型是什么？其对应的实际意义是什么？由此实际意义决定的实际数据类型又是什么？\n做数据分析时，我们常常将特征分为以下几种数据类型。\n（1）数值型数据\n数值型数据指具有数量上的意义，支持比较大小，同时还支持加减乘除、求算术平均等基本数学运算的那些数据。数值型数据在计算机中常常体现为整数或浮点数的存储形式。\n（2）排序型数据\n排序型数据也具有数量上的定义，所以支持比较大小，但不一定满足基本的数学运算。例如我们通常的排名数据，我们可以定义第1名最好，排名数字越大对应排名越差，所以排名是可以相互比较大小的。但是排名数据并不支持加法运算，例如第1名+第2名并不等于第3名，第1名与第3名的算术平均也并不一定就等于第2名，所以通常的排名，尽管可能以整数体现，但并不能当成线性域定义的数值来对待。\n排序型数据在计算机中可能出现整数和字符型的存储形式。\n（3）类别型数据\n类别型数据则没有数量上的对应性，因此没有大小的意义，仅仅作为一个标记符号，表示出一个类别与其他类别的不同，或者是一个样本与其他样本的不同。\n类别型数据在计算机中最常见的是字符型的存储形式，但也不乏以整数存储的情况。当出现以整数存储的类别数据时，要特别注意，尽管对其做数学运算不违背语法规则，但却是没有实际意义的。\n（4）逻辑型/布尔型数据\n类别型数据中，如果只存在两种非此即彼的选择，则一般称为逻辑型数据或布尔型数据。\n逻辑性数据在计算机中可以体现为布尔存储形式（True或False），也可能体现为整数0、1的存储形式。逻辑型数据支持的运算与数学运算不同，是我们所说的逻辑运算。\n例题5-1-2 Titanic数据集中的特征检查。\n结合各个特征的实际意义，我们会发现这12个特征呈现几种不同的数据类型。例如：\n特征‘年龄’‘同行平辈人数’‘同行父母或子女人数’‘船费’是有数量上的意义的，不同的值之间能比较大小，如‘人数’4大于‘人数’2，‘年龄’52大于‘年龄’34，也支持基本的数学运算，例如加减法、求算术平均，等等，因此，这些都是我们通常说的数值型的数据。\n特征‘性别’‘姓名’‘票号’‘登船港口’是以字符（串）存储型的类别型数据，不具备数量上的意义，不支持数学运算。\n特征‘是否生还’是什么类型？数据文件中各个样本在该特征上的取值是数字‘0’或‘1’，所以它是数值型数据吗？\n判断一个数据是否是数值型的，关键是考察其是否具备数量的意义（能比较大小），以及是否支持基本的数学运算。特征‘是否生还’有两个取值，1（是）、0（否），‘1’比‘0’大吗？这两种取值做加减法会有意义吗？显然这两个问题的答案都是否定的。所以它不是数值型数据。对于这种只有两个备选，且非此即彼的数据类型，我们通常称为逻辑型，或者布尔型。\n再来看特征‘仓位等级’，它是数值型吗？如果‘仓位等级’在定义的时候已经认定‘1’代表最好的仓位，数字越大仓位等级越低，那么这个特征有数量上的定义，可以比较大小。但是，一个1等舱加上一个2等仓，并不等于1个3等仓，所以它也不是一个数值型数据。我们可以认为这是一种排序类型，是依据某个度量排序得到的，具备一定的量化意义。如果这里的数字‘1’~‘4’仅仅代表了不同的类别，不具备量的意义，即数字大或小都不代表等级高，这时只能认为它属于类别型数据。\n最后，我们看一下特征‘乘客编号’，这个特征也极具迷惑性，好像是数值型或排序型。但是这个编号明显不支持数学运算，所以不是数值型。如果其大小没有特殊的含义，即不是某种度量的排序，那也不是排序型。该特征主要用来区别不同的乘客，所以我们依然可以把它当成类别型来对待。\n之前我们已经介绍过，pandas的read_csv函数将csv文件中的数据导入一个DataFrame结构，DataFrame封装的info函数，能提供一些对数据的基本分析，例如数据的规模、每个特征有多少非空值，以及特征存储时采用的数据类型。要注意的是，这里的存储数据类型并不一定是特征的实际意义上的数据类型，但能给我们提供一定参考。\n可见，对于特征，我们首先可依据‘是否具备量化意义（即是否可比较大小），是否支持数学运算’的准则，区分出数值型数据和非数值型数据。对于数值型数据，我们后续可以用算术平均、标准差等量化统计量来分析；而对于非数值型数据，我们则主要依据它们来进行分组与筛选，整个判断流程可以参照图5-1-1。\n图5-1-1 判断数据类型的流程\n例如，我们对上述加载的Titanic数据，根据‘仓位等级’进行分组，对‘船费’进行算术平均统计，就可以获得每种‘仓位等级’对应的平均‘船费’了。\n例5-1-3 Titanic数据集中不同仓位等级的船费统计。\nimport pandas as pd\nmy_data = pd.read_csv(\"Titanic.csv\")\nprint('Table 1. Mean Fare of Group')\nx=my_data.groupby(['PClass']).mean()\nprint(x ['Fare'])",
        "entities": [
            "数值型数据",
            "排序型数据",
            "类别型数据",
            "逻辑型/布尔型数据"
        ],
        "output": "这段文字介绍了特征的各种数据类型，包括：数值型数据、排序型数据、类别型数据、逻辑型/布尔型数据。返回为```json\n[\"数值型数据\", \"排序型数据\", \"类别型数据\", \"逻辑型/布尔型数据\"]\n```"
    },
    {
        "input": "7.2.2 展示可视化的三点基本原则\n具体到操作上，我们建议在展示阶段的可视化遵从以下三点基本原则。\n1.可视化的具体方法视我们要传递的信息而定\n我们通常的展示可大致分为对状态（由参数或数据描述）的展示、对过程的展示和对关系的展示（见图7-2-1）。其中对状态的展示又可能涉及时序变化展示、分布展示、（分组）对比展示，等等。\n图7-2-1 根据要传递的信息确定可视化方法（思维导图形式）\n时序变化展示是指展示某个参数随时间的变化。最基本的时序变化展示方式就是绘制时序图，也就是用X轴来代表时间，Y轴代表该量化参数，把参数随时间的演变曲线描记下来，如图7-2-2，就是描记的一位ICU病人连续三天中心率随一天的时间从早到晚变化的时序图。在一些高级可视化中，用一个具体的可变形象代表要表现的参数，然后采用动画的方式来直观表现其随时间的演化，有助于给观众留下深刻印象。\n图7-2-2 时序图举例\n（图中就1位患者不同3天中的心率，展示了其时序变化，横轴代表时间在一天中的不同时刻，纵轴则代表对应时刻测量得到的心率。三条不同颜色不同标记的曲线分别代表不同的三天。）\n图7-2-3 分布展示时的可视化方法选取\n分布展示是指要反映某个参数随其他参数的变化。例如常见的一种是样本发生概率（或样本出现频次）随某参数或参数区间的变化，联系第5章的描述性统计中的知识，直方图（如例5-3-5）是常用的一种展示方式。而如果要展示一个数值型参数随另一个或两个数值型参数的变化，或者同时体现样本在两个或三个数值型参数上的分布，则可以尝试散点图（如例5-3-9）描述，此时点的聚集趋势能反映出参数间的相互关系，而点的疏密程度能定性地反映出样本在不同参数区间中出现的频次。还有一种常见的情况是展示某个参数随空间的变化，例如样本出现频次随空间位置的变化，或其他非频次参数随空间位置的变化。此时，最直接的展示手段就是构造参数的分布地图，在有些领域也被称为地形图、热力图等。如图7-2-4中，将主要城市的位置在地图中用圆标记出来后，用圆内的不同颜色来表示各城市的空气质量指数（air quality index, AQI），色调从冷到暖对应着空气质量从好到坏，因此图中容易看出华北一带城市的空气质量比其他区域普遍要差一些的总体趋势。试想一下，如果不采用这种地图展示，而是直接将300多个城市的AQI用bar图在一张图中画出来，观众想要从中获得一个整体印象则要困难得多。总体而言，当要展示的数据条目非常多，而数据本身又有明确的坐标（位置）信息时，采用地图就是一种非常直观而高效的展示方式。\n图7-2-4 分布地图举例\n（图中用分布地图展示了中国300多个主要城市2015年的空气质量指数，每个小圆圈对应一个城市，其内的颜色则代表空气质量指数）\n对比展示是指要展示的信息不是分立的某个参数，而是参数在不同分组之间的比较结果，强调的是组间的对比关系和差异性（如果有的话）。前述的时序变化展示和分布展示都可以直接拿来进行组间对比，只要其本身能体现出足够醒目的组间差异。更简单地，则是对可比较大小的数值型参数进行组间比较，此时，第5章介绍过的柱状（bar）图、箱型图等都是不错的选择。而当对比的若干组总量恒定时，常见的饼图用来表示各组的占比及其排序关系也是一目了然。\n例7-2-1 用于比较的箱型图绘制举例。\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nmy_data = pd.read_csv(\"C:\\Python\\Scripts\\my_data\\german_credit_data_dataset.csv\")\nmy_dict={'A71':'unemployed', 'A72':'< 1 year', 'A73': '1 - 4 years',\n         'A74' : '4 - 7 years', 'A75' : '>= 7 years'}\nfig=plt.figure(figsize=(10,6))\nsns.set(style='whitegrid')\nsns.set_context(\"talk\")\nh=sns.boxplot(x='present_employment',y='credit_amount',data=my_data,\n           palette=sns.color_palette(\"ch:2.5,-.2,dark=.3\"),\n           linewidth=2,width=0.5,fliersize=6,\n           order=['A75','A74','A73','A72','A71'])\nax=plt.gca()\nax.set_xticklabels([my_dict['A75'],my_dict['A74'],my_dict['A73'],\n                    my_dict['A72'], my_dict['A71']],fontsize=15)\nax.set_xlabel('Employment Status')\nax.set_ylabel('Credit Amount (DM)')\nax.text(2.5,17600,'1000 Applicants in total',color='b')\n例7-2-2 考察1000个贷款申请客户的雇员状态占比并绘制饼图。\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nmy_data = pd.read_csv(\"C:\\Python\\Scripts\\my_data\\german_credit_data_dataset.csv\")\nplt_data=my_data[['duration','present_employment']].groupby('present_employment')\nshare = plt_data.count().values/plt_data.count().values.sum()\nlabels = ['Unemployed', '< 1 year', '1 - 4 years', '4 - 7 years', '>= 7 years']\nexplode = [0, 0, 0, 0, 0.1]\nfig = plt.figure(figsize=(6,5))\nplt.pie(share, explode = explode,\n        labels = labels, autopct = '%3.1f%%',\n        startangle = 180, shadow = True,\n        colors = ['red','tomato', 'yellowgreen', 'springgreen', 'lime'])\nplt.title(\"Employment status of 1000 applicants\")\nplt.show()\n除了对状态的可视化，对过程和关系的可视化展示常常也是需要的，此时不必局限于用代码编程来实现，而是可以借鉴各种方便的绘图工具或应用软件。\n过程的展示一般须包含构成完整过程的关键环节，并在图中明确表明各环节的先后顺序，以及各环节的关键输入与输出等。我们常见的流程图（见图4-5-2）、原理框图、数据流图（见图1-5-1）等，都可归为过程的可视化，在第6章介绍过的决策树的可视化也是一种过程的可视化展示。\n关系的展示则一般包含要研究的所有对象，并在这些对象间明确表示两两间的关系。网络图、层次图、树状图等都可以用来展示关系，如图7-2-5中的网络图，用点与点之间的连线的虚实来表示两点间关系的加强或减弱的变化，而具体的变化程度则用颜色来对应。此外，近年来流行的思维导图也可视为一种关系的可视化展示，如图7-2-1所示。\n图7-2-5 网络图举例\n（图中用网络图展示了不同节点间的某种相关关系在不同状态下的改变。节点间有连线即表示节点间的此种关系有明显改变，实现代表增强型改变，虚线代表减弱型改变，颜色则对应具体改变量的大小。）\n2.可视化要具备自明性，既包含图本身，也包含图中注解\n采用可视化手段后，我们一般不会再用大量的文字对图进行说明，此时若希望观众能迅速抓住图中传递的信息，必须在图中给与必要的注解。\n最基本的要求是，对于图中要表示的各种参数，都要明确标注。例如在二维坐标系中，X轴代表什么、Y轴代表什么（见图7-2-2）；两轴各自的单位（量纲），如果有，是什么；坐标轴上的刻度代表多大的数值范围。再例如，我们在地形图中，用不同颜色渲染所代表的参数是什么，其单位（如果有）是什么，颜色条（colorbar）对应的该参数的取值范围是什么，等等，都需要在图中明确标注（如图7-2-4空气质量地图）。\n然后是对图中分组信息的区分和标注。在进行分组对比时，为不引起混淆，常常用不同的颜色或标记符来区分不同的组，那么各种颜色、标记符各自对应哪个组，必须在图中给出注解，如图7-2-2时序图中用三种不同的颜色和标记符来区分不同的数据采集日期。\n3.合理选择一幅图中的信息容量和信息分辨率\n可视化的目的是快速地给观众留下深刻印象，成功与否还必须考虑人通常的在短时间之内的信息接受容限。一般而言，如果是非逻辑连贯的信息，一幅图中包含超过6个，观众要快速接受就存在困难了。例如，我们要对比用7个不同颜色（或标记符）来表示的7条时序曲线，如果这7条曲线之间并没有一个统一、连贯的关系，那这幅图在一般人看来就很难获取到其中的重点。近年来特别热门的思维导图，其本身作为一种供深入学习的组织与检索图是适宜的，但体系宏大、过于错综复杂的思维导图，作为展示手段，其能快速给观众的印象仅限于“哇哦，这是个复杂的体系”，如果想快速让观众获取其中细节信息，这种思维导图并不合适。同样的原则也适用于网络图。\n另一方面，在同样的区间（范围）内，现在的计算机表达数据的精细程度已远远超过人的视觉分辨率。所以，我们在可视化展示时，必须考虑到人眼本身的分辨力局限，如果想展示的细节超出了人眼分辨力的极限，那么这种展示也达不到预期效果。\n例7-2-3，以下是对ebay上出售的所有注册年份在2016年之前的德国二手车按40个品牌分类的价格信息展示图，你认为这是一种成功的可视化吗？\n解析：总体而言，上图不能算是成功的可视化，因为读者无法从中快速获得信息，甚至图中也根本没有尝试归纳出某种信息。其中未经逻辑组织的数据有40条，我们不仅不能明白这些数据之间的相互关系，即便是它们各自对应哪个品牌这种基本信息，读者也完全无法看清，因为x坐标轴上的品牌标注完全重叠在一起。如果真地希望包含完整的40个品牌的价格信息，分组也许是一种可能的解决方法。\n在改进的图中，我们把与其他品牌差异最大也是价格最高的4个品牌的价格（均值）单独用bar画出来了，其余36个品牌，依然依据价格划分为“高价”“中等”“低价”三组，分别用bar画出三组的平均价格，这样整幅图按价格降序排列提供总共7个数据，读者还是比较容易接受的。我们想突出的重点（4个顶级品牌，以及它们与其他36个品牌之间的巨大的价格差异）相信都会给读者留下印象。而其他36个品牌，我们可以根据图中标注的品牌分组，快速检索到该品牌处于什么价位，以及该品牌处于同等价位中的价格排名。至于smart到底比citroen价格贵出多少，相信在目前的价格区间中，体现真地不那么明显。\n从例7-2-3中我们可以体会到，结果展示时的可视化，首先要有清晰的展示逻辑，然后要让展示方式能凸显你想传递的逻辑信息。同时，尽管大数据时代，无论从数据层面还是工具层面，都可以支持大量信息的可视化，但作为展示手段，并不是信息包含越多就越好。\n本章中，首先介绍了面向不同对象时的结果展示逻辑，了解了面向不同对象时应有不同的侧重，例如面向出资方应侧重介绍项目结果，面向用户应侧重介绍项目成果如何能融入并改善用户的工作，面向数据科学家同行应侧重项目的关键技术、方法及结论。我们还专门就展示过程中的可视化进行了介绍，了解了可视化要兼顾科学与人两个层面，并建议了展示过程中可视化的基本原则，包括：①可视化的具体方法视我们要传递的信息而定；②可视化要具备自明性，既包含图本身，也包含图中注解；③合理选择一幅图中的信息容量和信息分辨率。恰当的展示能给人深刻的正面印象，起到积极的效果，所以其中的各种原则也是不容忽视。",
        "entities": [],
        "output": "这段文字介绍了展示可视化的三点基本原则，但是没有介绍一个新的概念或者引入新的名词，所以没有能够抽取出来的知识点。返回为```json\n[]\n```"
    },
    {
        "input": "4.4.6 Pandas. DataFrame\nPandas包中的DataFrame不要求元素的数据类型一致，而且完全可以等同于一个包含行索引和列标题的二维表格。\n例4-4-9（a） Pandas.DataFrame的创建举例。\nimport pandas as pd\nimport numpy as np\nmy_dataframe=pd.DataFrame(np.random.randn(4,5),\n                          index=['a','b','c','d'],\n                          columns=['A','B','C','D','E'])\nmy_dataframe\n例4-4-9（a）中，我们用Pandas的DataFrame命令创建了一个名为my_dataframe的数据框结构，内容是一个4行5列的表格，其行索引由参数index指定，是小写字母a~d，其列标题则由参数columns指定，是大写字母A~E，表格中4行5列的数据则用随机数来填充。\n例4-4-9（b）Pandas.DataFrame访问举例。\nmy_dataframe[['B','C']]   #指定列访问1，返回有列标题，返回值仍是dataframe\nmy_dataframe[['B']]  #指定列访问2，返回有列标题，返回值仍是dataframe\nmy_dataframe['B']  #指定列访问3，返回无列标题，返回值是序列series\nmy_dataframe.iloc[1] #指定行访问1，指定行号\nmy_dataframe.loc['b']  #指定行访问2，指定行索引\n对DataFrame的访问很有意思，指定行和指定列访问是不一样的。如例4-4-9（b）中所示，通过“数据框名[[列标题]]”的格式，可以获取指定列构成的子表（包含行序号和列标题），而“数据框名[列标题]”则获取指定列的内容，返回一个序列（含行序号，但无列标题）。而要访问指定行，我们则需要借助DataFrame内嵌的iloc方法或loc方法，其中iloc方法中指定的是行序号（数），而loc方法中指定的是行索引名（字符）。 \n总结一下，本节主要介绍了6个主要的数据结构，列表、元组、字典、集合、numpy的多维数组ndarray，以及pandas的数据框DataFrame，这些数据结构就是我们后续数据生成或导入、处理和分析的直接对象。",
        "entities": [
            "DataFrame"
        ],
        "output": "这段文字介绍了pnadas包中的DataFrame的创建和访问操作。返回为```json\n[\"DataFrame\"]\n```"
    },
    {
        "input": "1.2.2大数据\n数据量多大规模时能称之为大数据呢？其实，目前对于“大数据”并没有明确的规模界定。有观点认为，当数据量大到需要用并行计算工具处理时，即是大数据。但当我们定义大数据时，却常常不只看数据体量这一个维度。公认地，大数据具有通常所说的4V特点：\n(1)体量大（high volume）。\n(2)产生速度快而时效性高（high velocity）。数据增长速度快，同时数据具有高度的时效性，随着时间改变，数据的价值会很快流失，因而对数据处理速度的要求也很高，例如很多情况下都要求能进行实时分析。\n（3）类型繁多（high variety）。不仅仅是单一类型的数据，文本、语音、图像等不同媒介的数据，结构化、非结构化的数据全部混杂在一起。\n（4）高度真实而价值密度低（high veracity）。数据价值与数据体量之间没有正比例关系，大体量的数据中常常只蕴藏了很小的价值，例如24小时监控数据中有用的可能只有其中极少的部分，高度真实和冗余伴随的是低价值密度。\n在上述4V特点下，相应地大数据的分析处理也较传统数据分析有所改变。例如有部分观点认为大数据使我们面临数据的全体，而不再是部分或抽样；又如有人认为在大数据时代我们对不精确性的容忍度变得更高了，可接受一定的混杂性；再如大数据分析更着重刻画或寻找相关性，而非因果性。这些观点在应用层面有一定的适用性，但就科学层面而言，却并未获得学术届的一致认可。例如，我们获得的数据量比以往大了，但并不一定就真的是“全体”了，特别是，当前绝大多数的数据科学任务其根本都是从已知推测未知，既然存在“未知”，那么已知的就不是“全体”，所以这里所谓的“全体”，只是一个相对的概念；再如，现阶段我们可以只关注相关性，但如果可以做到，我们并不排斥考察因果性，或者说科学总是力图诠释因果的；最后，对于不精确性的容忍，其本质上是用数据的多样化和大体量带来的冗余来补偿的，当这种补偿达不到预期效果时，对数据精确性的要求也势必会提高。\n总之，随着“大数据”时代的深入，我们对于海量数据分析的理解也在不断尝试中调整，真正具备科学意义的本质概念与原则会在这一过程中沉淀下来。\n例1-2-1 相关性和因果性举例\n关于关注“相关”而非“因果”，我们不妨通过一个小故事来体会一下。1993年，美国学者艾格拉沃提出了Aprior算法，可以通过分析购物篮中的商品集合，找出商品间关系，然后研究或推测顾客的购买行为。沃尔玛公司很快就将Aprior算法引入了他们的POS机数据分析。此后，超市管理人员分析销售数据时发现了一个有趣的现象：有一些看似毫无联系的商品，例如“啤酒”和“尿布”，会经常出现在同一个购物篮中。于是，沃尔玛在布置卖场时就将这些体现出“相关性”的商品摆放在一起，并获得了商品销售收入的提升。在这里，是“啤酒”销售导致了“尿布”销售，亦或是反过来，都不是关注的重点，即不强调“因果性”。但既然两者有“相关性”，那就摆放在一起方便顾客拿取，以达到促进销售的目的。",
        "entities": [
            "大数据",
            "大数据的4V特点"
        ],
        "output": "这段文字介绍了的大数据的概念以及大数据的4V特点。返回为```json\n[\"大数据\", \"大数据的4V特点\"]\n```"
    },
    {
        "input": "1. 工具包的离线安装\n离线安装时，需要先将安装文件下载到本机。例如，在图4-2-3的页面上单击“Download files”链接，进入到文件列表（见图4-2-4），我们看到，包文件的后缀名是“whl”，俗称轮子文件。列表中包含了各种版本、适用于不同操作系统的包文件，选择你需要的版本下载存盘。安装时，在“命令提示符”程序中，切换工作目录到Python安装目录下的Scripts目录，然后在命令行输入“pip install” 及完整的工具包安装文件名，按回车键，即可进行安装了。\n图4-2-3 PYPI上的NumPy包页面截图\n图4-2-4 PYPI上的NumPy包下载文件列表页面截图",
        "entities": [],
        "output": "这段文字可能在介绍相应的工具包如何进行离线安装，但是没有介绍一个新的概念或者引入新的名词，所以没有能够抽取出来的知识点。返回为```json\n[]\n```"
    },
    {
        "input": "思考题\n就大数据时代数据分析的三大变革（“部分数据”到“全体数据”，“因果性”到“相关性”，“精确性”到“混杂性”），请谈谈你的体会与看法。\n对于150个鸢尾花数据（50个setosa、50个versicolor、50个virginica）实施“是setosa”和“不是setosa”的二分类，能实现总体判别accuracy为65%，请思考这个分类模型可接受吗？",
        "entities": [],
        "output": "这段文字是一个思考题，没有能够抽取出来的知识点。返回为```json\n[]\n```"
    },
    {
        "input": "6.1.4 p-hacking\n假设检验返回的p值是假设检验的重要结果之一，它反映的是基于当前空假设H0，现有样本计算出的检验统计量落入当前值及以外区域的概率，p值越小，表示这个概率越小，小到一定程度，例如小于我们设置的显著性水平，则可以拒绝H0。但是，需要提醒的是，此时我们拒绝空假设依然存在着错误拒绝的风险，错误拒绝的概率就是p。本质上，p反映的就是I型错误率，而反映的则是我们能接受的I型错误率的上限。\n再换一个角度，现在有一些数据分析工作，会尝试在大量特征中寻找（例如）的特征。这样找到的特征真地能说明统计学差异吗？事实上，即便H0成立，当我们随机尝试100种不同的特征时，出现5个落入拒绝域（）的特征也是完全合理的，因为这里0.05的意义本来就是H0为真时落入到拒绝域的概率。因此，我们一定要警惕这种过度挖掘数据的做法，这其实就是近些年引起关注和诟病的p-hacking（即p值操纵）。\n为避免p-hacking，传统统计学强调应先提假设，再做假设检验。例如，当想要看某个特征在两组之间有没有差别时，应先提H0：特征X在组A和组B上均值相同（或分布相同等），然后在做好混杂因素匹配的情况下进行数据准备和假设检验。而不是无目标地在大量特征中穷尽搜索，例如没有任何假设，把所有的特征甚至构造复合特征，逐个在组A、组B间检验后，挑出其中p值较小的来声称这些特征存在组间显著性差异。\n那么，是不是就不能考察大量特征了呢？也不尽然。我们认为最好的做法是用可重复性来验证，就是说，当分析一批数据找到p值小的特征后，先不要急于下结论，应针对该特征确定空假设，再充分搜集独立的新数据，并检查在新数据上这些专门针对该空假设的假设检验是否依然具有小的p值。在新数据上能重复的结论，才是可靠的。我们不要忘记，可重复性始终是区分科学与伪科学的一条重要原则。",
        "entities": [
            "假设检验",
            "p值",
            "p-hacking"
        ],
        "output": "这段文字介绍了p值的意义，以及需要警惕数据过度挖掘的做法，即p-hacking和避免p-hacking的方法。返回为```json\n[\"假设检验\", \"p值\", \"p-hacking\"]\n```"
    },
    {
        "input": "3.1.4 确定数据构成\n通过了可行性分析之后，就可以确定数据的构成了。通常而言，我们后续方便处理的数据都是“结构化”的数据。结构化的数据可以理解为一张不能再细分的二维表，表中一行代表一个存在且唯一的样本，一列代表一个属性。例如，例3-1-2的方案A中，一架飞机就是一个样本，飞机各部位的弹孔密度、飞机总体残损程度则是相应的属性。对一架具体的飞机，其对应的总体残损程度和各部位弹孔密度分别填入一行中的各列，不同的飞机填入不同的行，如表3-1-1所示，这样就构造出飞机数据的二维表格，也就是结构化的数据了。\n表3-1-1 例3-1-3中方案A的结构化数据\n我们再看一下降低银行不良贷款率任务的例子。第2章中，我们已明确这是一个对申请贷款的用户做出“普通客户”/“高风险客户”二分类的任务。接下来，你会提出怎样的前提假设？如果你认为放贷风险与申请客户的历史信用、贷款目的、贷款金额、还款能力等有关系，那你能确定你需要哪些数据吗？对于这些数据，你有切实可行的获取方法吗？确定好可行的数据方案后，我们构造一张申请贷款的客户资料二维表格（见表3-1-2），表格中每一行代表一位客户，每一列则反映一个我们想要获取的客户数据，例如历史信用、贷款期数（月）、贷款目的（购车、购房、教育还是普通消费）、贷款金额、储蓄或理财账户的余额、可支配月收入与月还款比、房产、担保、抚养赡养人数、职业、婚姻状况、已有贷款金额和月供，等等，这样，贷款甄别任务的数据构成就确定了。\n表3-1-2 贷款客户甄别任务中的结构化数据\n总结一下，在明确任务之后，我们需要提出前提假设。不同的假设很可能会涉及不同的数据，最终导致不同的研究内容，因此，在真正进入数据收集环节之前，应认真审视，确定与事实最符合的假设。然后，根据前提假设，给出对应的数据方案，并认真思考获取方案中数据的可行性，如果方案中的数据是在项目期限内无法获得的，就必须及时做出调整。最后，根据确定下来的数据方案，就可以确定出数据构成了。",
        "entities": [
            "结构化数据"
        ],
        "output": "这段文字介绍了结构化数据的定义以及如何确定数据的构成。返回为```json\n[\"结构化数据\"]\n```"
    }
]
[
    {
        "text": "后面我们会介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weightdecay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习的方法。通过减小权重参数的值来抑制过拟合的发生。\n如果想减小权重的值，一开始就将初始值设为较小的值才是正途。实际上，在这之前的权重初始值都是像0.01 * np.random.randn(10, 100)这样，使用由高斯分布生成的值乘以0.01后得到的值（标准差为0.01的高斯分布）。\n如果我们把权重初始值全部设为0以减小权重的值，会怎么样呢？从结论来说，将权重初始值设为0不是一个好主意。事实上，将权重初始值设为0的话，将无法正确进行学习。\n为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新（回忆一下“乘法节点的反向传播”\n的内容）。因此，权重被更新为相同的值，并拥有了对称的值（重复的值）这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。",
        "relations": [
            {
                "head": "过拟合",
                "relation": "相关",
                "tail": "权值衰减"
            }
        ],
        "entities": [
            "过拟合",
            "权值衰减",
            "权重均一化"
        ],
        "input": "实体列表为: [\"过拟合\", \"权值衰减\", \"权重均一化\"] 文本片段为: 后面我们会介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weightdecay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习的方法。通过减小权重参数的值来抑制过拟合的发生。\n如果想减小权重的值，一开始就将初始值设为较小的值才是正途。实际上，在这之前的权重初始值都是像0.01 * np.random.randn(10, 100)这样，使用由高斯分布生成的值乘以0.01后得到的值（标准差为0.01的高斯分布）。\n如果我们把权重初始值全部设为0以减小权重的值，会怎么样呢？从结论来说，将权重初始值设为0不是一个好主意。事实上，将权重初始值设为0的话，将无法正确进行学习。\n为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新（回忆一下“乘法节点的反向传播”\n的内容）。因此，权重被更新为相同的值，并拥有了对称的值（重复的值）这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。",
        "output": "```json\n[{\"head\": \"过拟合\", \"relation\": \"相关\", \"tail\": \"权值衰减\"}]\n```"
    },
    {
        "text": "本书提供了im2col函数，并将这个im2col函数作为黑盒（不关心内部实使用。im2col的实现内容在common/util.py中，它的实现（实质上）是一个行左右的简单函数。有兴趣的读者可以参考。\nim2col这一便捷函数具有以下接口。\nim2col (input_data, filter_h, filter_w, stride=1, pad=0)• input_data―由（数据量，通道，高，长）的4维数组构成的输入数据• filter_h―滤波器的高• filter_w―滤波器的长• stride―步幅• pad―填充\nim2col会考虑滤波器大小、步幅、填充，将输入数据展开为2维数组。\n现在，我们来实际使用一下这个im2col。\nimport sys, ossys.path.append(os.pardir)from common.util import im2colx1 = np.random.rand(1, 3, 7, 7)col1 = im2col(x1, 5, 5, stride=1, pad=0)print(col1.shape) # (9, 75)x2 = np.random.rand(10, 3, 7, 7) # 10个数据col2 = im2col(x2, 5, 5, stride=1, pad=0)print(col2.shape) # (90, 75)这里举了两个例子。第一个是批大小为1、通道为3的7 × 7的数据，第二个的批大小为10，数据形状和第一个相同。分别对其应用im2col函数，在这两种情形下，第2维的元素个数均为75。这是滤波器（通道为3、大小为5 × 5）的元素个数的总和。批大小为1时，im2col的结果是(9, 75)。而第2个例子中批大小为10，所以保存了10倍的数据，即(90, 75)。\n现在使用im2col来实现卷积层。这里我们将卷积层实现为名为Convolution的类。\nclass Convolution:    def __init__(self, W, b, stride=1, pad=0):        self.W = W        self.b = b        self.stride = stride        self.pad = pad    def forward(self, x):        FN, C, FH, FW = self.W.shape        N, C, H, W = x.shape        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)        col = im2col(x, FH, FW, self.stride, self.pad)        col_W = self.W.reshape(FN, -1).T # 滤波器的展开        out = np.dot(col, col_W) + self.b        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)        return out\n卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。\n滤波器是(FN, C, FH, FW) 的4 维形状。另外，FN、C、FH、FW 分别是FilterNumber（滤波器数量）、Channel、Filter Height、Filter Width的缩写。\n这里用粗体字表示Convolution层的实现中的重要部分。在这些粗体字部分，用im2col展开输入数据，并用reshape将滤波器展开为2维数组。然后，计算展开后的矩阵的乘积。\n展开滤波器的部分（代码段中的粗体字）如图7-19所示，将各个滤波器的方块纵向展开为1 列。这里通过reshape(FN,-1)将参数指定为-1，这是reshape的一个便利的功能。通过在reshape时指定为-1，reshape函数会自动计算-1维度上的元素个数，以使多维数组的元素个数前后一致。比如，(10, 3, 5, 5)形状的数组的元素个数共有750个，指定reshape(10,-1)后，就会转换成(10, 75)形状的数组。\nforward的实现中，最后会将输出大小转换为合适的形状。转换时使用了NumPy的transpose函数。transpose会更改多维数组的轴的顺序。如图7-20所示，通过指定从0开始的索引（编号）序列，就可以更改轴的顺序。\n以上就是卷积层的forward处理的实现。通过使用im2col进行展开，基本上可以像实现全连接层的Affine层一样来实现（5.6节）。接下来是卷积层的反向传播的实现，因为和Affine层的实现有很多共通的地方，所以就不再介绍了。但有一点需要注意，在进行卷积层的反向传播时，必须进行im2col的逆处理。这可以使用本书提供的col2im函数（col2im的实现在common/util.\ny中）来进行。除了使用col2im这一点，卷积层的反向传播和Affi  ne层的实现方式都一样。卷积层的反向传播的实现在common/layer.py中，有兴趣的读者可以参考。",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 本书提供了im2col函数，并将这个im2col函数作为黑盒（不关心内部实使用。im2col的实现内容在common/util.py中，它的实现（实质上）是一个行左右的简单函数。有兴趣的读者可以参考。\nim2col这一便捷函数具有以下接口。\nim2col (input_data, filter_h, filter_w, stride=1, pad=0)• input_data―由（数据量，通道，高，长）的4维数组构成的输入数据• filter_h―滤波器的高• filter_w―滤波器的长• stride―步幅• pad―填充\nim2col会考虑滤波器大小、步幅、填充，将输入数据展开为2维数组。\n现在，我们来实际使用一下这个im2col。\nimport sys, ossys.path.append(os.pardir)from common.util import im2colx1 = np.random.rand(1, 3, 7, 7)col1 = im2col(x1, 5, 5, stride=1, pad=0)print(col1.shape) # (9, 75)x2 = np.random.rand(10, 3, 7, 7) # 10个数据col2 = im2col(x2, 5, 5, stride=1, pad=0)print(col2.shape) # (90, 75)这里举了两个例子。第一个是批大小为1、通道为3的7 × 7的数据，第二个的批大小为10，数据形状和第一个相同。分别对其应用im2col函数，在这两种情形下，第2维的元素个数均为75。这是滤波器（通道为3、大小为5 × 5）的元素个数的总和。批大小为1时，im2col的结果是(9, 75)。而第2个例子中批大小为10，所以保存了10倍的数据，即(90, 75)。\n现在使用im2col来实现卷积层。这里我们将卷积层实现为名为Convolution的类。\nclass Convolution:    def __init__(self, W, b, stride=1, pad=0):        self.W = W        self.b = b        self.stride = stride        self.pad = pad    def forward(self, x):        FN, C, FH, FW = self.W.shape        N, C, H, W = x.shape        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)        col = im2col(x, FH, FW, self.stride, self.pad)        col_W = self.W.reshape(FN, -1).T # 滤波器的展开        out = np.dot(col, col_W) + self.b        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)        return out\n卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。\n滤波器是(FN, C, FH, FW) 的4 维形状。另外，FN、C、FH、FW 分别是FilterNumber（滤波器数量）、Channel、Filter Height、Filter Width的缩写。\n这里用粗体字表示Convolution层的实现中的重要部分。在这些粗体字部分，用im2col展开输入数据，并用reshape将滤波器展开为2维数组。然后，计算展开后的矩阵的乘积。\n展开滤波器的部分（代码段中的粗体字）如图7-19所示，将各个滤波器的方块纵向展开为1 列。这里通过reshape(FN,-1)将参数指定为-1，这是reshape的一个便利的功能。通过在reshape时指定为-1，reshape函数会自动计算-1维度上的元素个数，以使多维数组的元素个数前后一致。比如，(10, 3, 5, 5)形状的数组的元素个数共有750个，指定reshape(10,-1)后，就会转换成(10, 75)形状的数组。\nforward的实现中，最后会将输出大小转换为合适的形状。转换时使用了NumPy的transpose函数。transpose会更改多维数组的轴的顺序。如图7-20所示，通过指定从0开始的索引（编号）序列，就可以更改轴的顺序。\n以上就是卷积层的forward处理的实现。通过使用im2col进行展开，基本上可以像实现全连接层的Affine层一样来实现（5.6节）。接下来是卷积层的反向传播的实现，因为和Affine层的实现有很多共通的地方，所以就不再介绍了。但有一点需要注意，在进行卷积层的反向传播时，必须进行im2col的逆处理。这可以使用本书提供的col2im函数（col2im的实现在common/util.\ny中）来进行。除了使用col2im这一点，卷积层的反向传播和Affi  ne层的实现方式都一样。卷积层的反向传播的实现在common/layer.py中，有兴趣的读者可以参考。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "现在我们试着用上述的数值微分对简单函数进行求导。先来看一个由下式表示的2次函数。\n>>> numerical_diff(function_1, 5)0.1999999999990898>>> numerical_diff(function_1, 10)0.2999999999986347\n这里计算的导数是f(x) 相对于x 的变化量，对应函数的斜率。另外，f(x) = 0.01x2 + 0.1x 的解析解是。因此，在x = 5 和x = 10处，“真的导数”分别为0.2和0.3。和上面的结果相比，我们发现虽然严格意义上它们并不一致，但误差非常小。实际上，误差小到基本上可以认为它们是相等的。\n现在，我们用上面的数值微分的值作为斜率，画一条直线。结果如图4-7所示，可以确认这些直线确实对应函数的切线（源代码在ch04/gradient_1d.\npy中）。",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 现在我们试着用上述的数值微分对简单函数进行求导。先来看一个由下式表示的2次函数。\n>>> numerical_diff(function_1, 5)0.1999999999990898>>> numerical_diff(function_1, 10)0.2999999999986347\n这里计算的导数是f(x) 相对于x 的变化量，对应函数的斜率。另外，f(x) = 0.01x2 + 0.1x 的解析解是。因此，在x = 5 和x = 10处，“真的导数”分别为0.2和0.3。和上面的结果相比，我们发现虽然严格意义上它们并不一致，但误差非常小。实际上，误差小到基本上可以认为它们是相等的。\n现在，我们用上面的数值微分的值作为斜率，画一条直线。结果如图4-7所示，可以确认这些直线确实对应函数的切线（源代码在ch04/gradient_1d.\npy中）。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算（NumPy 中是np.dot()，具体请参照3.3 节）。比如，还记得我们用Python进行了下面的实现吗？\n>>> X = np.random.rand(2)   # 输入>>> W = np.random.rand(2,3) # 权重>>> B = np.random.rand(3)   # 偏置>>>>>> X.shape # (2,)>>> W.shape # (2, 3)>>> B.shape # (3,)>>>>>> Y = np.dot(X, W) + B这里，X、W、B 分别是形状为(2,)、(2, 3)、(3,)的多维数组。这样一来，神经元的加权和可以用Y = np.dot(X, W) + B计算出来。然后，Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。此外，我们来复习一下，矩阵的乘积运算的要点是使对应维度的元素个数一致。比如，如下面的图5-23所示，X和W 的乘积必须使对应维度的元素个数一致。\n另外，这里矩阵的形状用(2, 3)这样的括号表示（为了和NumPy的shape属性的输出一致）。\n>>> X = np.random.rand(2)   # 输入>>> W = np.random.rand(2,3) # 权重>>> B = np.random.rand(3)   # 偏置>>>>>> X.shape # (2,)>>> W.shape # (2, 3)>>> B.shape # (3,)>>>>>> Y = np.dot(X, W) + B\n神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”A。因此，这里将进行仿射变换的处理实现为“Affine层”。\n现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。\n将乘积运算用“dot”节点表示的话，则np.dot(X, W) + B的运算可用图5-24所示的计算图表示出来。另外，在各个变量的上方标记了它们的形状（比如，计算图上显示了X的形状为(2,)，X·W的形状为(3,)等）。\n图5-24是比较简单的计算图，不过要注意X、W、B是矩阵（多维数组）。\n之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。\n现在我们来考虑图5-24的计算图的反向传播。以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。实际写一下的话，可以得到下式（这里省略了式（5.13）的推导过程）。\n（5.13）\n式（5.13）中W T的T表示转置。转置操作会把W的元素(i, j)换成元素j, i)。用数学式表示的话，可以写成下面这样。\n（5.14）\n如式（5.14）所示，如果W的形状是(2, 3)，W T的形状就是(3, 2)。\n现在，我们根据式（5.13），尝试写出计算图的反向传播，如图5-25所示。\n我们看一下图2-25的计算图中各个变量的形状。尤其要注意，X和形状相同，W和形状相同。从下面的数学式可以很明确地看出X和形状相同。\n（5.15）\n为什么要注意矩阵的形状呢？因为矩阵的乘积运算要求对应维度的元素个数保持一致，通过确认一致性，就可以导出式（5.13）。比如，的形状是(3,)，W的形状是(2, 3)时，思考和W T的乘积，使得的形状为(2,)（图5-26）。这样一来，就会自然而然地推导出式（5.13）。",
        "relations": [
            {
                "head": "仿射变换",
                "relation": "相关",
                "tail": "Affine层"
            }
        ],
        "entities": [
            "正向传播",
            "仿射变换",
            "Affine层"
        ],
        "input": "实体列表为: [\"正向传播\", \"仿射变换\", \"Affine层\"] 文本片段为: 神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算（NumPy 中是np.dot()，具体请参照3.3 节）。比如，还记得我们用Python进行了下面的实现吗？\n>>> X = np.random.rand(2)   # 输入>>> W = np.random.rand(2,3) # 权重>>> B = np.random.rand(3)   # 偏置>>>>>> X.shape # (2,)>>> W.shape # (2, 3)>>> B.shape # (3,)>>>>>> Y = np.dot(X, W) + B这里，X、W、B 分别是形状为(2,)、(2, 3)、(3,)的多维数组。这样一来，神经元的加权和可以用Y = np.dot(X, W) + B计算出来。然后，Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。此外，我们来复习一下，矩阵的乘积运算的要点是使对应维度的元素个数一致。比如，如下面的图5-23所示，X和W 的乘积必须使对应维度的元素个数一致。\n另外，这里矩阵的形状用(2, 3)这样的括号表示（为了和NumPy的shape属性的输出一致）。\n>>> X = np.random.rand(2)   # 输入>>> W = np.random.rand(2,3) # 权重>>> B = np.random.rand(3)   # 偏置>>>>>> X.shape # (2,)>>> W.shape # (2, 3)>>> B.shape # (3,)>>>>>> Y = np.dot(X, W) + B\n神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”A。因此，这里将进行仿射变换的处理实现为“Affine层”。\n现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。\n将乘积运算用“dot”节点表示的话，则np.dot(X, W) + B的运算可用图5-24所示的计算图表示出来。另外，在各个变量的上方标记了它们的形状（比如，计算图上显示了X的形状为(2,)，X·W的形状为(3,)等）。\n图5-24是比较简单的计算图，不过要注意X、W、B是矩阵（多维数组）。\n之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。\n现在我们来考虑图5-24的计算图的反向传播。以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。实际写一下的话，可以得到下式（这里省略了式（5.13）的推导过程）。\n（5.13）\n式（5.13）中W T的T表示转置。转置操作会把W的元素(i, j)换成元素j, i)。用数学式表示的话，可以写成下面这样。\n（5.14）\n如式（5.14）所示，如果W的形状是(2, 3)，W T的形状就是(3, 2)。\n现在，我们根据式（5.13），尝试写出计算图的反向传播，如图5-25所示。\n我们看一下图2-25的计算图中各个变量的形状。尤其要注意，X和形状相同，W和形状相同。从下面的数学式可以很明确地看出X和形状相同。\n（5.15）\n为什么要注意矩阵的形状呢？因为矩阵的乘积运算要求对应维度的元素个数保持一致，通过确认一致性，就可以导出式（5.13）。比如，的形状是(3,)，W的形状是(2, 3)时，思考和W T的乘积，使得的形状为(2,)（图5-26）。这样一来，就会自然而然地推导出式（5.13）。",
        "output": "```json\n[{\"head\": \"仿射变换\", \"relation\": \"相关\", \"tail\": \"Affine层\"}]\n```"
    },
    {
        "text": "异或门也被称为逻辑异或电路。如图2-5所示，仅当x1或x2中的一方为1时，才会输出1（“异或”是拒绝其他的意思）。那么，要用感知机实现这个异或门的话，应该设定什么样的权重参数呢？\n实际上，用前面介绍的感知机是无法实现这个异或门的。为什么用感知机可以实现与门、或门，却无法实现异或门呢？下面我们尝试通过画图来思考其中的原因。\n首先，我们试着将或门的动作形象化。或门的情况下，当权重参数(b, w1, w2) = (−0.5, 1.0, 1.0)时，可满足图2-4的真值表条件。此时，感知机可用下面的式（2.3）表示。\n（2.3）\n式（2.3）表示的感知机会生成由直线−0.5 + x1 + x2 = 0分割开的两个空间。其中一个空间输出1，另一个空间输出0，如图2-6所示。\n或门在(x1, x2) = (0, 0) 时输出0，在(x1, x2) 为(0, 1)、(1, 0)、(1, 1) 时输出1。图2-6中，○表示0，△表示1。如果想制作或门，需要用直线将图2-\n中的○和△分开。实际上，刚才的那条直线就将这4个点正确地分开了。\n那么，换成异或门的话会如何呢？能否像或门那样，用一条直线作出分割图2-7中的○和△的空间呢？\n想要用一条直线将图2-7中的○和△分开，无论如何都做不到。事实上，用一条直线是无法将○和△分开的。",
        "relations": [],
        "entities": [
            "异或门"
        ],
        "input": "实体列表为: [\"异或门\"] 文本片段为: 异或门也被称为逻辑异或电路。如图2-5所示，仅当x1或x2中的一方为1时，才会输出1（“异或”是拒绝其他的意思）。那么，要用感知机实现这个异或门的话，应该设定什么样的权重参数呢？\n实际上，用前面介绍的感知机是无法实现这个异或门的。为什么用感知机可以实现与门、或门，却无法实现异或门呢？下面我们尝试通过画图来思考其中的原因。\n首先，我们试着将或门的动作形象化。或门的情况下，当权重参数(b, w1, w2) = (−0.5, 1.0, 1.0)时，可满足图2-4的真值表条件。此时，感知机可用下面的式（2.3）表示。\n（2.3）\n式（2.3）表示的感知机会生成由直线−0.5 + x1 + x2 = 0分割开的两个空间。其中一个空间输出1，另一个空间输出0，如图2-6所示。\n或门在(x1, x2) = (0, 0) 时输出0，在(x1, x2) 为(0, 1)、(1, 0)、(1, 1) 时输出1。图2-6中，○表示0，△表示1。如果想制作或门，需要用直线将图2-\n中的○和△分开。实际上，刚才的那条直线就将这4个点正确地分开了。\n那么，换成异或门的话会如何呢？能否像或门那样，用一条直线作出分割图2-7中的○和△的空间呢？\n想要用一条直线将图2-7中的○和△分开，无论如何都做不到。事实上，用一条直线是无法将○和△分开的。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "虽然通过GPU可以实现深度学习运算的高速化，但即便如此，当网络较深时，学习还是需要几天到几周的时间。并且，前面也说过，深度学习伴随着很多试错。为了创建良好的网络，需要反复进行各种尝试，这样一来就必然会产生尽可能地缩短一次学习所需的时间的要求。于是，将深度学习的学习过程扩展开来的想法（也就是分布式学习）就变得重要起来。\n为了进一步提高深度学习所需的计算的速度，可以考虑在多个GPU或\n者多台机器上进行分布式计算。现在的深度学习框架中，出现了好几个支持多GPU或者多机器的分布式学习的框架。其中，Google的TensorFlow、微软的CNTK（Computational Network Toolki）在开发过程中高度重视分布式学习。以大型数据中心的低延迟·高吞吐网络作为支撑，基于这些框架的分布式学习呈现出惊人的效果。\n基于分布式学习，可以达到何种程度的高速化呢？图8-16中显示了基于TensorFlow的分布式学习的效果。\n如图8-16所示，随着GPU个数的增加，学习速度也在提高。实际上，与使用1 个GPU 时相比，使用100 个GPU（设置在多台机器上，共100 个）似乎可以实现56倍的高速化！这意味着之前花费7天的学习只要3个小时就能完成，充分说明了分布式学习惊人的效果。\n关于分布式学习，“如何进行分布式计算”是一个非常难的课题。它包含了机器间的通信、数据的同步等多个无法轻易解决的问题。可以将这些难题都交给TensorFlow等优秀的框架。这里，我们不讨论分布式学习的细节。\n关于分布式学习的技术性内容，请参考TensorFlow的技术论文（白皮书）等。",
        "relations": [],
        "entities": [
            "分布式学习"
        ],
        "input": "实体列表为: [\"分布式学习\"] 文本片段为: 虽然通过GPU可以实现深度学习运算的高速化，但即便如此，当网络较深时，学习还是需要几天到几周的时间。并且，前面也说过，深度学习伴随着很多试错。为了创建良好的网络，需要反复进行各种尝试，这样一来就必然会产生尽可能地缩短一次学习所需的时间的要求。于是，将深度学习的学习过程扩展开来的想法（也就是分布式学习）就变得重要起来。\n为了进一步提高深度学习所需的计算的速度，可以考虑在多个GPU或\n者多台机器上进行分布式计算。现在的深度学习框架中，出现了好几个支持多GPU或者多机器的分布式学习的框架。其中，Google的TensorFlow、微软的CNTK（Computational Network Toolki）在开发过程中高度重视分布式学习。以大型数据中心的低延迟·高吞吐网络作为支撑，基于这些框架的分布式学习呈现出惊人的效果。\n基于分布式学习，可以达到何种程度的高速化呢？图8-16中显示了基于TensorFlow的分布式学习的效果。\n如图8-16所示，随着GPU个数的增加，学习速度也在提高。实际上，与使用1 个GPU 时相比，使用100 个GPU（设置在多台机器上，共100 个）似乎可以实现56倍的高速化！这意味着之前花费7天的学习只要3个小时就能完成，充分说明了分布式学习惊人的效果。\n关于分布式学习，“如何进行分布式计算”是一个非常难的课题。它包含了机器间的通信、数据的同步等多个无法轻易解决的问题。可以将这些难题都交给TensorFlow等优秀的框架。这里，我们不讨论分布式学习的细节。\n关于分布式学习的技术性内容，请参考TensorFlow的技术论文（白皮书）等。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "图像分割是指在像素水平上对图像进行分类。如图8-19所示，使用以像素为单位对各个对象分别着色的监督数据进行学习。然后，在推理时，对输入图像的所有像素进行分类。\n之前实现的神经网络是对图像整体进行了分类，要将它落实到像素水平的话，该怎么做呢？\n要基于神经网络进行图像分割，最简单的方法是以所有像素为对象，对每个像素执行推理处理。比如，准备一个对某个矩形区域中心的像素进行分类的网络，以所有像素为对象执行推理处理。正如大家能想到的，这样的方法需要按照像素数量进行相应次forward处理，因而需要耗费大量的时间（正确地说，卷积运算中会发生重复计算很多区域的无意义的计算）。为了解决这个无意义的计算问题，有人提出了一个名为FCN（Fully ConvolutionalNetwork）[37]的方法。该方法通过一次forward处理，对所有像素进行分类（图8-20）。\nFCN的字面意思是 “全部由卷积层构成的网络”。相对于一般的CNN包含全连接层，FCN将全连接层替换成发挥相同作用的卷积层。在物体识别中使用的网络的全连接层中，中间数据的空间容量被作为排成一列的节点进\n行处理，而只由卷积层构成的网络中，空间容量可以保持原样直到最后的输出。\n如图8-20所示，FCN的特征在于最后导入了扩大空间大小的处理。基于这个处理，变小了的中间数据可以一下子扩大到和输入图像一样的大小。\nFCN最后进行的扩大处理是基于双线性插值法的扩大（双线性插值扩大）。\nFCN中，这个双线性插值扩大是通过去卷积（逆卷积运算）来实现的（细节请参考FCN的论文[37]）。\n全连接层中，输出和全部的输入相连。使用卷积层也可以实现与此结构完全相同的连接。比如，针对输入大小是32×10×10（通道数32、高10、长10）的数据的全连接层可以替换成滤波器大小为32×10×10的卷积层。如果全连接层的输出节点数是100，那么在卷积层准备100个32×10×10的滤波器就可以实现完全相同的处理。\n像这样，全连接层可以替换成进行相同处理的卷积层。",
        "relations": [
            {
                "head": "图像分割",
                "relation": "包含",
                "tail": "FCN"
            }
        ],
        "entities": [
            "图像分割",
            "FCN"
        ],
        "input": "实体列表为: [\"图像分割\", \"FCN\"] 文本片段为: 图像分割是指在像素水平上对图像进行分类。如图8-19所示，使用以像素为单位对各个对象分别着色的监督数据进行学习。然后，在推理时，对输入图像的所有像素进行分类。\n之前实现的神经网络是对图像整体进行了分类，要将它落实到像素水平的话，该怎么做呢？\n要基于神经网络进行图像分割，最简单的方法是以所有像素为对象，对每个像素执行推理处理。比如，准备一个对某个矩形区域中心的像素进行分类的网络，以所有像素为对象执行推理处理。正如大家能想到的，这样的方法需要按照像素数量进行相应次forward处理，因而需要耗费大量的时间（正确地说，卷积运算中会发生重复计算很多区域的无意义的计算）。为了解决这个无意义的计算问题，有人提出了一个名为FCN（Fully ConvolutionalNetwork）[37]的方法。该方法通过一次forward处理，对所有像素进行分类（图8-20）。\nFCN的字面意思是 “全部由卷积层构成的网络”。相对于一般的CNN包含全连接层，FCN将全连接层替换成发挥相同作用的卷积层。在物体识别中使用的网络的全连接层中，中间数据的空间容量被作为排成一列的节点进\n行处理，而只由卷积层构成的网络中，空间容量可以保持原样直到最后的输出。\n如图8-20所示，FCN的特征在于最后导入了扩大空间大小的处理。基于这个处理，变小了的中间数据可以一下子扩大到和输入图像一样的大小。\nFCN最后进行的扩大处理是基于双线性插值法的扩大（双线性插值扩大）。\nFCN中，这个双线性插值扩大是通过去卷积（逆卷积运算）来实现的（细节请参考FCN的论文[37]）。\n全连接层中，输出和全部的输入相连。使用卷积层也可以实现与此结构完全相同的连接。比如，针对输入大小是32×10×10（通道数32、高10、长10）的数据的全连接层可以替换成滤波器大小为32×10×10的卷积层。如果全连接层的输出节点数是100，那么在卷积层准备100个32×10×10的滤波器就可以实现完全相同的处理。\n像这样，全连接层可以替换成进行相同处理的卷积层。",
        "output": "```json\n[{\"head\": \"图像分割\", \"relation\": \"包含\", \"tail\": \"FCN\"}]\n```"
    },
    {
        "text": "在一个标题为“What is the class of this image ?”的网站[32]上，以排行榜的形式刊登了目前为止通过论文等渠道发表的针对各种数据集的方法的识别精度（图8-3）。\n观察图8-3 的排行结果，可以发现“Neural Networks” “Deep” “Convolutional”等关键词特别显眼。实际上，排行榜上的前几名大都是基于CNN的方法。顺便说一下，截止到2016年6月，对MNIST数据集的最高识别精度是99.79%（错误识别率为0.21%），该方法也是以CNN为基础的[33]。\n不过，它用的CNN并不是特别深层的网络（卷积层为2层、全连接层为2层的网络）。\n对于MNIST数据集，层不用特别深就获得了（目前）最高的识别精度。一般认为，这是因为对于手写数字识别这样一个比较简单的任务，没有必要将网络的表现力提高到那么高的程度。因此，可以说加深层的好处并不大。而之后要介绍的大规模的一般物体识别的情况，因为问题复杂，所以加深层对提高识别精度大有裨益。\n参考刚才排行榜中前几名的方法，可以发现进一步提高识别精度的技术和线索。比如，集成学习、学习率衰减、Data Augmentation（数据扩充）等都有助于提高识别精度。尤其是Data Augmentation，虽然方法很简单，但在提高识别精度上效果显著。\nData Augmentation基于算法“人为地”扩充输入图像（训练图像）。具体地说，如图8-4所示，对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。这在数据集的图像数量有限时尤其有效。\n除了如图8-4所示的变形之外，Data Augmentation还可以通过其他各种方法扩充图像，比如裁剪图像的 “crop处理”、将图像左右翻转的“fl ip处理”A 等。对于一般的图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。不管怎样，通过Data Augmentation巧妙地增加训练图像，就可以提高深度学习的识别精度。虽然这个看上去只是一个简单的技巧，不过经常会有很好的效果。这里，我们不进行Data Augmentation的实现，不\n过这个技巧的实现比较简单，有兴趣的读者请自己试一下。",
        "relations": [],
        "entities": [
            "集成学习",
            "学习率衰减",
            "Data Augmentation"
        ],
        "input": "实体列表为: [\"集成学习\", \"学习率衰减\", \"Data Augmentation\"] 文本片段为: 在一个标题为“What is the class of this image ?”的网站[32]上，以排行榜的形式刊登了目前为止通过论文等渠道发表的针对各种数据集的方法的识别精度（图8-3）。\n观察图8-3 的排行结果，可以发现“Neural Networks” “Deep” “Convolutional”等关键词特别显眼。实际上，排行榜上的前几名大都是基于CNN的方法。顺便说一下，截止到2016年6月，对MNIST数据集的最高识别精度是99.79%（错误识别率为0.21%），该方法也是以CNN为基础的[33]。\n不过，它用的CNN并不是特别深层的网络（卷积层为2层、全连接层为2层的网络）。\n对于MNIST数据集，层不用特别深就获得了（目前）最高的识别精度。一般认为，这是因为对于手写数字识别这样一个比较简单的任务，没有必要将网络的表现力提高到那么高的程度。因此，可以说加深层的好处并不大。而之后要介绍的大规模的一般物体识别的情况，因为问题复杂，所以加深层对提高识别精度大有裨益。\n参考刚才排行榜中前几名的方法，可以发现进一步提高识别精度的技术和线索。比如，集成学习、学习率衰减、Data Augmentation（数据扩充）等都有助于提高识别精度。尤其是Data Augmentation，虽然方法很简单，但在提高识别精度上效果显著。\nData Augmentation基于算法“人为地”扩充输入图像（训练图像）。具体地说，如图8-4所示，对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。这在数据集的图像数量有限时尤其有效。\n除了如图8-4所示的变形之外，Data Augmentation还可以通过其他各种方法扩充图像，比如裁剪图像的 “crop处理”、将图像左右翻转的“fl ip处理”A 等。对于一般的图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。不管怎样，通过Data Augmentation巧妙地增加训练图像，就可以提高深度学习的识别精度。虽然这个看上去只是一个简单的技巧，不过经常会有很好的效果。这里，我们不进行Data Augmentation的实现，不\n过这个技巧的实现比较简单，有兴趣的读者请自己试一下。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流/不流”（1/0）两种取值。在本书中，0对应“不传递信号”，1对应“传递信号”。\n图2-1是一个接收两个输入信号的感知机的例子。x1、x2是输入信号，y 是输出信号，w1、w2 是权重（w 是weight 的首字母）。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重\nw1x1、w2x2）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。\n（2.1）\n感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。\n权重相当于电流里所说的电阻。电阻是决定电流流动难度的参数，电阻越低，通过的电流就越大。而感知机的权重则是值越大，通过的信号就越大。不管是电阻还是权重，在控制信号流动难度（或者流动容易度）这一点上的作用都是一样的。",
        "relations": [
            {
                "head": "感知机",
                "relation": "包含",
                "tail": "权重"
            }
        ],
        "entities": [
            "感知机",
            "权重"
        ],
        "input": "实体列表为: [\"感知机\", \"权重\"] 文本片段为: 感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流/不流”（1/0）两种取值。在本书中，0对应“不传递信号”，1对应“传递信号”。\n图2-1是一个接收两个输入信号的感知机的例子。x1、x2是输入信号，y 是输出信号，w1、w2 是权重（w 是weight 的首字母）。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重\nw1x1、w2x2）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。\n（2.1）\n感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。\n权重相当于电流里所说的电阻。电阻是决定电流流动难度的参数，电阻越低，通过的电流就越大。而感知机的权重则是值越大，通过的信号就越大。不管是电阻还是权重，在控制信号流动难度（或者流动容易度）这一点上的作用都是一样的。",
        "output": "```json\n[{\"head\": \"感知机\", \"relation\": \"包含\", \"tail\": \"权重\"}]\n```"
    },
    {
        "text": "下面，我们来介绍矩阵（二维数组）的乘积。比如2 × 2的矩阵，其乘积可以像图3-11这样进行计算（按图中顺序进行计算是规定好了的）。\n如本例所示，矩阵的乘积是通过左边矩阵的行（横向）和右边矩阵的列（纵向）以对应元素的方式相乘后再求和而得到的。并且，运算的结果保存为新的多维数组的元素。比如，A的第1行和B的第1列的乘积结果是新数组的第1行第1列的元素，A的第2行和B的第1列的结果是新数组的第2行第1列的元素。另外，在本书的数学标记中，矩阵将用黑斜体表示（比如，矩阵A），以区别于单个元素的标量（比如，a或b）。这个运算在Python中可以用如下代码实现。\n>>> A = np.array([[1,2], [3,4]])\n>>> A.shape(2, 2)>>> B = np.array([[5,6], [7,8]])>>> B.shape(2, 2)>>> np.dot(A, B)array([[19, 22],       [43, 50]])这里，A 和B 都是2 × 2 的矩阵，它们的乘积可以通过NumPy 的np.dot()函数计算（乘积也称为点积）。np.dot()接收两个NumPy数组作为参数，并返回数组的乘积。这里要注意的是，np.dot(A, B)和np.dot(B, A)的值可能不一样。和一般的运算（+或*等）不同，矩阵的乘积运算中，操作数（A、B）的顺序不同，结果也会不同。\n这里介绍的是计算2 × 2形状的矩阵的乘积的例子，其他形状的矩阵的乘积也可以用相同的方法来计算。比如，2 × 3的矩阵和3 × 2 的矩阵的乘积可按如下形式用Python来实现。\n>>> A = np.array([[1,2,3], [4,5,6]])>>> A.shape(2, 3)>>> B = np.array([[1,2], [3,4], [5,6]])>>> B.shape(3, 2)>>> np.dot(A, B)array([[22, 28],       [49, 64]])2 × 3的矩阵A和3 × 2的矩阵B的乘积可按以上方式实现。这里需要注意的是矩阵的形状（shape）。具体地讲，矩阵A的第1维的元素个数（列数）必须和矩阵B的第0维的元素个数（行数）相等。在上面的例子中，矩阵A的形状是2 × 3，矩阵B的形状是3 × 2，矩阵A的第1维的元素个数（3）和矩阵B的第0维的元素个数（3）相等。如果这两个值不相等，则无法计算矩阵的乘积。比如，如果用Python计算2 × 3 的矩阵A和2 × 2的矩阵C的乘积，则会输出如下错误。\n>>> C = np.array([[1,2], [3,4]])>>> C.shape\n(2, 2)>>> A.shape(2, 3)>>> np.dot(A, C)Traceback (most recent call last):  File \"<stdin>\", line 1, in <module>ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)这个错误的意思是，矩阵A的第1维和矩阵C的第0维的元素个数不一致（维度的索引从0开始）。也就是说，在多维数组的乘积运算中，必须使两个矩阵中的对应维度的元素个数一致，这一点很重要。我们通过图3-12再来确认一下。\n这个错误的意思是，矩阵A的第1维和矩阵C的第0维的元素个数不一致（维度的索引从0开始）。也就是说，在多维数组的乘积运算中，必须使两个矩阵中的对应维度的元素个数一致，这一点很重要。我们通过图3-12再来确认一下。\n图3-12中，3 × 2的矩阵A和2 × 4 的矩阵B的乘积运算生成了3 × 4的矩阵C。如图所示，矩阵A和矩阵B的对应维度的元素个数必须保持一致。\n此外，还有一点很重要，就是运算结果的矩阵C的形状是由矩阵A的行数和矩阵B的列数构成的。\n另外，当A是二维矩阵、B是一维数组时，如图3-13所示，对应维度的元素个数要保持一致的原则依然成立。\n可按如下方式用Python实现图3-13的例子。",
        "relations": [
            {
                "head": "矩阵乘法",
                "relation": "顺序",
                "tail": "np.dot()"
            }
        ],
        "entities": [
            "矩阵乘法",
            "np.dot()"
        ],
        "input": "实体列表为: [\"矩阵乘法\", \"np.dot()\"] 文本片段为: 下面，我们来介绍矩阵（二维数组）的乘积。比如2 × 2的矩阵，其乘积可以像图3-11这样进行计算（按图中顺序进行计算是规定好了的）。\n如本例所示，矩阵的乘积是通过左边矩阵的行（横向）和右边矩阵的列（纵向）以对应元素的方式相乘后再求和而得到的。并且，运算的结果保存为新的多维数组的元素。比如，A的第1行和B的第1列的乘积结果是新数组的第1行第1列的元素，A的第2行和B的第1列的结果是新数组的第2行第1列的元素。另外，在本书的数学标记中，矩阵将用黑斜体表示（比如，矩阵A），以区别于单个元素的标量（比如，a或b）。这个运算在Python中可以用如下代码实现。\n>>> A = np.array([[1,2], [3,4]])\n>>> A.shape(2, 2)>>> B = np.array([[5,6], [7,8]])>>> B.shape(2, 2)>>> np.dot(A, B)array([[19, 22],       [43, 50]])这里，A 和B 都是2 × 2 的矩阵，它们的乘积可以通过NumPy 的np.dot()函数计算（乘积也称为点积）。np.dot()接收两个NumPy数组作为参数，并返回数组的乘积。这里要注意的是，np.dot(A, B)和np.dot(B, A)的值可能不一样。和一般的运算（+或*等）不同，矩阵的乘积运算中，操作数（A、B）的顺序不同，结果也会不同。\n这里介绍的是计算2 × 2形状的矩阵的乘积的例子，其他形状的矩阵的乘积也可以用相同的方法来计算。比如，2 × 3的矩阵和3 × 2 的矩阵的乘积可按如下形式用Python来实现。\n>>> A = np.array([[1,2,3], [4,5,6]])>>> A.shape(2, 3)>>> B = np.array([[1,2], [3,4], [5,6]])>>> B.shape(3, 2)>>> np.dot(A, B)array([[22, 28],       [49, 64]])2 × 3的矩阵A和3 × 2的矩阵B的乘积可按以上方式实现。这里需要注意的是矩阵的形状（shape）。具体地讲，矩阵A的第1维的元素个数（列数）必须和矩阵B的第0维的元素个数（行数）相等。在上面的例子中，矩阵A的形状是2 × 3，矩阵B的形状是3 × 2，矩阵A的第1维的元素个数（3）和矩阵B的第0维的元素个数（3）相等。如果这两个值不相等，则无法计算矩阵的乘积。比如，如果用Python计算2 × 3 的矩阵A和2 × 2的矩阵C的乘积，则会输出如下错误。\n>>> C = np.array([[1,2], [3,4]])>>> C.shape\n(2, 2)>>> A.shape(2, 3)>>> np.dot(A, C)Traceback (most recent call last):  File \"<stdin>\", line 1, in <module>ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)这个错误的意思是，矩阵A的第1维和矩阵C的第0维的元素个数不一致（维度的索引从0开始）。也就是说，在多维数组的乘积运算中，必须使两个矩阵中的对应维度的元素个数一致，这一点很重要。我们通过图3-12再来确认一下。\n这个错误的意思是，矩阵A的第1维和矩阵C的第0维的元素个数不一致（维度的索引从0开始）。也就是说，在多维数组的乘积运算中，必须使两个矩阵中的对应维度的元素个数一致，这一点很重要。我们通过图3-12再来确认一下。\n图3-12中，3 × 2的矩阵A和2 × 4 的矩阵B的乘积运算生成了3 × 4的矩阵C。如图所示，矩阵A和矩阵B的对应维度的元素个数必须保持一致。\n此外，还有一点很重要，就是运算结果的矩阵C的形状是由矩阵A的行数和矩阵B的列数构成的。\n另外，当A是二维矩阵、B是一维数组时，如图3-13所示，对应维度的元素个数要保持一致的原则依然成立。\n可按如下方式用Python实现图3-13的例子。",
        "output": "```json\n[{\"head\": \"矩阵乘法\", \"relation\": \"顺序\", \"tail\": \"np.dot()\"}]\n```"
    },
    {
        "text": "阶跃函数和sigmoid函数还有其他共同点，就是两者均为非线性函数。\nsigmoid函数是一条曲线，阶跃函数是一条像阶梯一样的折线，两者都属于非线性的函数。\n在介绍激活函数时，经常会看到“非线性函数”和“线性函数”等术语。\n函数本来是输入某个值后会返回一个值的转换器。向这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学式表示为h(x) = cx。c为常数）。因此，线性函数是一条笔直的直线。\n而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。\n神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。\n线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。为了具体地（稍微直观地）理解这一点，我们来思考下面这个简单的例子。这里我们考虑把线性函数h(x) = cx 作为激活函数，把y(x) = h(h(h(x))) 的运算对应3 层神经网络A。这个运算会进行y(x) = c × c × c × x的乘法运算，但是同样的处理可以由y(x) = ax（注意，a = c 3）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示，使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。",
        "relations": [
            {
                "head": "非线性函数",
                "relation": "相关",
                "tail": "线性函数"
            },
            {
                "head": "非线性函数",
                "relation": "包含",
                "tail": "阶跃函数"
            },
            {
                "head": "非线性函数",
                "relation": "包含",
                "tail": "sigmoid函数"
            }
        ],
        "entities": [
            "阶跃函数",
            "sigmod函数",
            "线性函数",
            "非线性函数"
        ],
        "input": "实体列表为: [\"阶跃函数\", \"sigmod函数\", \"线性函数\", \"非线性函数\"] 文本片段为: 阶跃函数和sigmoid函数还有其他共同点，就是两者均为非线性函数。\nsigmoid函数是一条曲线，阶跃函数是一条像阶梯一样的折线，两者都属于非线性的函数。\n在介绍激活函数时，经常会看到“非线性函数”和“线性函数”等术语。\n函数本来是输入某个值后会返回一个值的转换器。向这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学式表示为h(x) = cx。c为常数）。因此，线性函数是一条笔直的直线。\n而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。\n神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。\n线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。为了具体地（稍微直观地）理解这一点，我们来思考下面这个简单的例子。这里我们考虑把线性函数h(x) = cx 作为激活函数，把y(x) = h(h(h(x))) 的运算对应3 层神经网络A。这个运算会进行y(x) = c × c × c × x的乘法运算，但是同样的处理可以由y(x) = ax（注意，a = c 3）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示，使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。",
        "output": "```json\n[{\"head\": \"非线性函数\", \"relation\": \"相关\", \"tail\": \"线性函数\"}, {\"head\": \"非线性函数\", \"relation\": \"包含\", \"tail\": \"阶跃函数\"}, {\"head\": \"非线性函数\", \"relation\": \"包含\", \"tail\": \"sigmoid函数\"}]\n```"
    },
    {
        "text": "NumPy不仅可以生成一维数组（排成一列的数组），也可以生成多维数组比如，可以生成如下的二维数组（矩阵）。\n比如，可以生成如下的二维数组（矩阵）。\n>>> A = np.array([[1, 2], [3, 4]])>>> print(A)[[1 2] [3 4]]>>> A.shape(2, 2)>>> A.dtypedtype('int64')这里生成了一个2 × 2的矩阵A。另外，矩阵A的形状可以通过shape查看，矩阵元素的数据类型可以通过dtype查看。\n下面，我们来看一下矩阵的算术运算。\n>>> B = np.array([[3, 0],[0, 6]])>>> A + B\n>>> B = np.array([[3, 0],[0, 6]])>>> A + Barray([[ 4,  2],       [ 3, 10]])>>> A * Barray([[ 3,  0],       [ 0, 24]])和数组的算术运算一样，矩阵的算术运算也可以在相同形状的矩阵间以对应元素的方式进行。并且，也可以通过标量（单一数值）对矩阵进行算术运算。\n这也是基于广播的功能。\n和数组的算术运算一样，矩阵的算术运算也可以在相同形状的矩阵间以对应元素的方式进行。并且，也可以通过标量（单一数值）对矩阵进行算术运算这也是基于广播的功能。\nNumPy数组（np.array）可以生成N维数组，即可以生成一维数组、二维数组、三维数组等任意维数的数组。数学上将一维数组称为向量，将二维数组称为矩阵。另外，可以将一般化之后的向量或矩阵等统称为张量（tensor）。本书基本上将二维数组称为“矩阵”，将三维数组及三维以上的数组称为“张量”或“多维数组”。",
        "relations": [
            {
                "head": "Numpy",
                "relation": "相关",
                "tail": "多维数组"
            },
            {
                "head": "多维数组",
                "relation": "包含",
                "tail": "矩阵"
            },
            {
                "head": "多维数组",
                "relation": "包含",
                "tail": "张量"
            },
            {
                "head": "多维数组",
                "relation": "相关",
                "tail": "广播"
            }
        ],
        "entities": [
            "Numpy",
            "多维数组",
            "矩阵",
            "张量",
            "广播"
        ],
        "input": "实体列表为: [\"Numpy\", \"多维数组\", \"矩阵\", \"张量\", \"广播\"] 文本片段为: NumPy不仅可以生成一维数组（排成一列的数组），也可以生成多维数组比如，可以生成如下的二维数组（矩阵）。\n比如，可以生成如下的二维数组（矩阵）。\n>>> A = np.array([[1, 2], [3, 4]])>>> print(A)[[1 2] [3 4]]>>> A.shape(2, 2)>>> A.dtypedtype('int64')这里生成了一个2 × 2的矩阵A。另外，矩阵A的形状可以通过shape查看，矩阵元素的数据类型可以通过dtype查看。\n下面，我们来看一下矩阵的算术运算。\n>>> B = np.array([[3, 0],[0, 6]])>>> A + B\n>>> B = np.array([[3, 0],[0, 6]])>>> A + Barray([[ 4,  2],       [ 3, 10]])>>> A * Barray([[ 3,  0],       [ 0, 24]])和数组的算术运算一样，矩阵的算术运算也可以在相同形状的矩阵间以对应元素的方式进行。并且，也可以通过标量（单一数值）对矩阵进行算术运算。\n这也是基于广播的功能。\n和数组的算术运算一样，矩阵的算术运算也可以在相同形状的矩阵间以对应元素的方式进行。并且，也可以通过标量（单一数值）对矩阵进行算术运算这也是基于广播的功能。\nNumPy数组（np.array）可以生成N维数组，即可以生成一维数组、二维数组、三维数组等任意维数的数组。数学上将一维数组称为向量，将二维数组称为矩阵。另外，可以将一般化之后的向量或矩阵等统称为张量（tensor）。本书基本上将二维数组称为“矩阵”，将三维数组及三维以上的数组称为“张量”或“多维数组”。",
        "output": "```json\n[{\"head\": \"Numpy\", \"relation\": \"相关\", \"tail\": \"多维数组\"}, {\"head\": \"多维数组\", \"relation\": \"包含\", \"tail\": \"矩阵\"}, {\"head\": \"多维数组\", \"relation\": \"包含\", \"tail\": \"张量\"}, {\"head\": \"多维数组\", \"relation\": \"相关\", \"tail\": \"广播\"}]\n```"
    },
    {
        "text": "除了单一的数值，还可以用列表（数组）汇总数据。\n>>> a = [1, 2, 3, 4, 5] # 生成列表>>> print(a)  # 输出列表的内容[1, 2, 3, 4, 5]>>> len(a)    # 获取列表的长度5>>> a[0]      # 访问第一个元素的值1>>> a[4]5>>> a[4] = 99 # 赋值>>> print(a)[1, 2, 3, 4, 99]\n元素的访问是通过a[0]这样的方式进行的。[]中的数字称为索引（下标），索引从0 开始（索引0 对应第一个元素）。此外，Python 的列表提供了切片（slicing）这一便捷的标记法。使用切片不仅可以访问某个值，还可以访问列表的子列表（部分列表）。\n>>> print(a)[1, 2, 3, 4, 99]>>> a[0:2] # 获取索引为0到2（不包括2！\n）的元素[1, 2]>>> a[1:]  # 获取从索引为1的元素到最后一个元素[2, 3, 4, 99]\n>>> a[:3]  # 获取从第一个元素到索引为3（不包括3！\n）的元素[1, 2, 3]>>> a[:-1] # 获取从第一个元素到最后一个元素的前一个元素之间的元素[1, 2, 3, 4]>>> a[:-2] # 获取从第一个元素到最后一个元素的前二个元素之间的元素[1, 2, 3]\n进行列表的切片时，需要写成a[0:2]这样的形式。a[0:2]用于取出从索引为0的元素到索引为2的元素的前一个元素之间的元素。另外，索引−1对应最后一个元素，−2对应最后一个元素的前一个元素。",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 除了单一的数值，还可以用列表（数组）汇总数据。\n>>> a = [1, 2, 3, 4, 5] # 生成列表>>> print(a)  # 输出列表的内容[1, 2, 3, 4, 5]>>> len(a)    # 获取列表的长度5>>> a[0]      # 访问第一个元素的值1>>> a[4]5>>> a[4] = 99 # 赋值>>> print(a)[1, 2, 3, 4, 99]\n元素的访问是通过a[0]这样的方式进行的。[]中的数字称为索引（下标），索引从0 开始（索引0 对应第一个元素）。此外，Python 的列表提供了切片（slicing）这一便捷的标记法。使用切片不仅可以访问某个值，还可以访问列表的子列表（部分列表）。\n>>> print(a)[1, 2, 3, 4, 99]>>> a[0:2] # 获取索引为0到2（不包括2！\n）的元素[1, 2]>>> a[1:]  # 获取从索引为1的元素到最后一个元素[2, 3, 4, 99]\n>>> a[:3]  # 获取从第一个元素到索引为3（不包括3！\n）的元素[1, 2, 3]>>> a[:-1] # 获取从第一个元素到最后一个元素的前一个元素之间的元素[1, 2, 3, 4]>>> a[:-2] # 获取从第一个元素到最后一个元素的前二个元素之间的元素[1, 2, 3]\n进行列表的切片时，需要写成a[0:2]这样的形式。a[0:2]用于取出从索引为0的元素到索引为2的元素的前一个元素之间的元素。另外，索引−1对应最后一个元素，−2对应最后一个元素的前一个元素。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "可以使用matplotlib的pyplot模块绘制图形。话不多说，我们来看一个绘制sin函数曲线的例子。\nimport numpy as npimport matplotlib.pyplot as plt\nplt.plot(x, y)plt.show()这里使用NumPy的arange方法生成了[0, 0.1, 0.2, ���, 5.8, 5.9]的数据，将其设为x。对x的各个元素，应用NumPy的sin函数np.sin()，将x、y的数据传给plt.plot方法，然后绘制图形。最后，通过plt.show()显示图形。\n运行上述代码后，就会显示图1-3所示的图形。",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 可以使用matplotlib的pyplot模块绘制图形。话不多说，我们来看一个绘制sin函数曲线的例子。\nimport numpy as npimport matplotlib.pyplot as plt\nplt.plot(x, y)plt.show()这里使用NumPy的arange方法生成了[0, 0.1, 0.2, ���, 5.8, 5.9]的数据，将其设为x。对x的各个元素，应用NumPy的sin函数np.sin()，将x、y的数据传给plt.plot方法，然后绘制图形。最后，通过plt.show()显示图形。\n运行上述代码后，就会显示图1-3所示的图形。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "如果有人问你现在有多幸福，你会如何回答呢？一般的人可能会给出诸如“还可以吧”或者“不是那么幸福”等笼统的回答。如果有人回答“我现在的幸福指数是10.23”的话，可能会把人吓一跳吧。因为他用一个数值指标来评判自己的幸福程度。这里的幸福指数只是打个比方，实际上神经网络的学习也在做同样的事情。\n神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。和刚刚那位以幸福指数为指引寻找“最优人生”的人一样，神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。\n损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。",
        "relations": [],
        "entities": [
            "损失函数"
        ],
        "input": "实体列表为: [\"损失函数\"] 文本片段为: 如果有人问你现在有多幸福，你会如何回答呢？一般的人可能会给出诸如“还可以吧”或者“不是那么幸福”等笼统的回答。如果有人回答“我现在的幸福指数是10.23”的话，可能会把人吓一跳吧。因为他用一个数值指标来评判自己的幸福程度。这里的幸福指数只是打个比方，实际上神经网络的学习也在做同样的事情。\n神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。和刚刚那位以幸福指数为指引寻找“最优人生”的人一样，神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。\n损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。遗憾的是，神经网络的最优化问题非常难。这是因为参数空间非常复杂，无法轻易找到最优解（无法使用那种通过解数学式一下子就求得最小值的方法）。\n而且，在深度神经网络中，参数的数量非常庞大，导致最优化问题更加复杂。在前几章中，为了找到最优参数，我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称SGD。\nSGD是一个简单的方法，不过比起胡乱地搜索参数空间，也算是“聪明”的方法。但是，根据不同的问题，也存在比SGD更加聪明的方法。本节我们将指出SGD的缺点，并介绍SGD以外的其他最优化方法。",
        "relations": [
            {
                "head": "最优化",
                "relation": "包含",
                "tail": "随机梯度下降法"
            }
        ],
        "entities": [
            "最优化",
            "随机梯度下降法"
        ],
        "input": "实体列表为: [\"最优化\",\"随机梯度下降法\"] 文本片段为: 神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。遗憾的是，神经网络的最优化问题非常难。这是因为参数空间非常复杂，无法轻易找到最优解（无法使用那种通过解数学式一下子就求得最小值的方法）。\n而且，在深度神经网络中，参数的数量非常庞大，导致最优化问题更加复杂。在前几章中，为了找到最优参数，我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称SGD。\nSGD是一个简单的方法，不过比起胡乱地搜索参数空间，也算是“聪明”的方法。但是，根据不同的问题，也存在比SGD更加聪明的方法。本节我们将指出SGD的缺点，并介绍SGD以外的其他最优化方法。",
        "output": "```json\n[{\"head\": \"最优化\", \"relation\": \"包含\", \"tail\": \"随机梯度下降法\"}]\n```"
    },
    {
        "text": "顺便提一下，在图3-2的网络中，偏置b并没有被画出来。如果要明确地表示出b，可以像图3-3那样做。图3-3中添加了权重为b的输入信号1。这个感知机将x1、x2、1三个信号作为神经元的输入，将其和各自的权重相乘后，传送至下一个神经元。在下一个神经元中，计算这些加权信号的总和。\n如果这个总和超过0，则输出1，否则输出0。另外，由于偏置的输入信号一直是1，所以为了区别于其他神经元，我们在图中把这个神经元整个涂成灰色。",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 顺便提一下，在图3-2的网络中，偏置b并没有被画出来。如果要明确地表示出b，可以像图3-3那样做。图3-3中添加了权重为b的输入信号1。这个感知机将x1、x2、1三个信号作为神经元的输入，将其和各自的权重相乘后，传送至下一个神经元。在下一个神经元中，计算这些加权信号的总和。\n如果这个总和超过0，则输出1，否则输出0。另外，由于偏置的输入信号一直是1，所以为了区别于其他神经元，我们在图中把这个神经元整个涂成灰色。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "绝大多数的模型使用，都分为模型建立（建模、或模型训练）和模型应用两个阶段，如图6-2-4所示。在建模阶段，主要是根据已有的数据确定好模型的参数，如回归系数等。这一过程，在例6-2-3和例6-2-1中体现为对模型实例的fit函数的调用。\n我们建立模型的目的总是为了将模型应用于未知数据，这一应用过程中，通常不会再修改模型的参数，而是将模型作为一个确定的函数，给它输入一个新的数据，期待模型输出对于该数据的预测或分类结果。这一过程，在例6-2-3和例6-2-1中体现为对模型实例的predict函数的调用。",
        "relations": [
            {
                "head": "模型建立",
                "relation": "顺序",
                "tail": "模型应用"
            }
        ],
        "entities": [
            "模型建立",
            "模型应用"
        ],
        "input": "实体列表为: [\"模型建立\",\"模型应用\"] 文本片段为: 绝大多数的模型使用，都分为模型建立（建模、或模型训练）和模型应用两个阶段，如图6-2-4所示。在建模阶段，主要是根据已有的数据确定好模型的参数，如回归系数等。这一过程，在例6-2-3和例6-2-1中体现为对模型实例的fit函数的调用。\n我们建立模型的目的总是为了将模型应用于未知数据，这一应用过程中，通常不会再修改模型的参数，而是将模型作为一个确定的函数，给它输入一个新的数据，期待模型输出对于该数据的预测或分类结果。这一过程，在例6-2-3和例6-2-1中体现为对模型实例的predict函数的调用。",
        "output": "```json\n[{\"head\": \"模型建立\", \"relation\": \"顺序\", \"tail\": \"模型应用\"}]\n```"
    },
    {
        "text": "如果偏差和方差是两个独立因素，那么，不管任何场合，我们只管追求两个因素都尽可能小就行了。然而遗憾的是，我们之前的推导中，并没有能证明两者之间的独立性。恰恰相反地，现实情况中，偏差与方差总是有着一定的矛盾性，如果追求低偏差，往往就得到高的方差，要追求低方差，则常常偏差又会很大。这就是我们说的偏差-方差困境。",
        "relations": [
            {
                "head": "偏差-方差困境",
                "relation": "包含",
                "tail": "偏差"
            },
            {
                "head": "偏差-方差困境",
                "relation": "包含",
                "tail": "方差"
            }
        ],
        "entities": [
            "偏差-方差困境",
            "偏差",
            "方差"
        ],
        "input": "实体列表为: [\"偏差-方差困境\",\"偏差\",\"方差\"] 文本片段为: 如果偏差和方差是两个独立因素，那么，不管任何场合，我们只管追求两个因素都尽可能小就行了。然而遗憾的是，我们之前的推导中，并没有能证明两者之间的独立性。恰恰相反地，现实情况中，偏差与方差总是有着一定的矛盾性，如果追求低偏差，往往就得到高的方差，要追求低方差，则常常偏差又会很大。这就是我们说的偏差-方差困境。",
        "output": "```json\n[{\"head\": \"偏差-方差困境\", \"relation\": \"包含\", \"tail\": \"偏差\"},{\"head\": \"偏差-方差困境\", \"relation\": \"包含\", \"tail\": \"方差\"}]\n```"
    },
    {
        "text": "6.2.2 线性回归模型性能评价\n例6-2-1（a）演示了如何利用sklearn库中的现成工具LinearRegression来实现线性回归。不知大家是否注意到，我们在用其中的fit函数时，其实除了要求自变量x和因变量y等长，以及该长度（即训练集样本容量）大于1，再没有任何其他限制。也就是说，只要有等长的且序列长度超过1的x和y，就一定能找到一个回归模型。但是，这个模型到底是好还是不好呢？或者，从数学的角度来说，只要我们能给出一定量的样本，利用最小二乘法总是能找到一条回归线（例如图6-7的左、右两幅子图中，都找到了绿色实线代表的回归直线）。但是，这条回归线只代表了所有同阶次线中与样本间RMSE最小的一条，并不代表这个RMSE就是一个可接受的RMSE。例如例6-2-1（a）中，没有一个比较标准，很难确定约0.40的RMSE是好还是不好的。\n图6-2-2 回归模型总是能找到\n那么，怎么评价我们的回归模型好与不好呢？或者说：我们真地能有效利用这条回归线去通过x预测y吗；更直白地，得到的回归有意义吗？我们必须引入一个回归效果的评价参数，即决定系数。\n我们可借助图6-2-3来做几个定义。\n图6-2-3 的意义\n对所有的样本，定义总平方和为：\n     （6-2-8）\n它代表样本相对于样本均值的总离差平方和。结合式（6-2-5）定义的残差平方和，可给出的定义，即\n    （6-2-9）\n进一步还可定义回归平方和为：\n  （6-2-10）\n它代表由于回归函数所引入的样本相对于样本均值的离差，属于样本变异性中可以被回归模型解释的部分。数学上可证明，最小二乘法确定的回归模型中，\n    （6-2-11）\n因此也可以由：\n    （6-2-12）\n求得。根据以上定义，衡量的是数据总变异性（总平方和）中可由模型解释的变异性（回归平方和）所占的比例，其取值在0到1之间，越靠近1，则数据中可由模型解释的成分越多，从而代表模型性能越好；越接近0，则数据中的模型不可解释成分越多，模型性能越不好。\n例6-2-1（b） 线性回归评价举例。\nprint('r_square = ',linreg.score(x,y))\nLinearRegression.score方法能直接返回。我们看到6-2-1（a）中回归模型约0.76，可见并不是特别好。",
        "relations": [
            {
                "head": "线性回归",
                "relation": "包含",
                "tail": "LinearRegression"
            },
            {
                "head": "线性回归",
                "relation": "包含",
                "tail": "RMSE"
            },
            {
                "head": "线性回归",
                "relation": "相关",
                "tail": "决定系数"
            }
        ],
        "entities": [
            "LinearRegression",
            "线性回归",
            "RMSE",
            "决定系数"
        ],
        "input": "实体列表为: [\"LinearRegression\", \"线性回归\", \"RMSE\", \"决定系数\"] 文本片段为: 6.2.2 线性回归模型性能评价\n例6-2-1（a）演示了如何利用sklearn库中的现成工具LinearRegression来实现线性回归。不知大家是否注意到，我们在用其中的fit函数时，其实除了要求自变量x和因变量y等长，以及该长度（即训练集样本容量）大于1，再没有任何其他限制。也就是说，只要有等长的且序列长度超过1的x和y，就一定能找到一个回归模型。但是，这个模型到底是好还是不好呢？或者，从数学的角度来说，只要我们能给出一定量的样本，利用最小二乘法总是能找到一条回归线（例如图6-7的左、右两幅子图中，都找到了绿色实线代表的回归直线）。但是，这条回归线只代表了所有同阶次线中与样本间RMSE最小的一条，并不代表这个RMSE就是一个可接受的RMSE。例如例6-2-1（a）中，没有一个比较标准，很难确定约0.40的RMSE是好还是不好的。\n图6-2-2 回归模型总是能找到\n那么，怎么评价我们的回归模型好与不好呢？或者说：我们真地能有效利用这条回归线去通过x预测y吗；更直白地，得到的回归有意义吗？我们必须引入一个回归效果的评价参数，即决定系数。\n我们可借助图6-2-3来做几个定义。\n图6-2-3 的意义\n对所有的样本，定义总平方和为：\n     （6-2-8）\n它代表样本相对于样本均值的总离差平方和。结合式（6-2-5）定义的残差平方和，可给出的定义，即\n    （6-2-9）\n进一步还可定义回归平方和为：\n  （6-2-10）\n它代表由于回归函数所引入的样本相对于样本均值的离差，属于样本变异性中可以被回归模型解释的部分。数学上可证明，最小二乘法确定的回归模型中，\n    （6-2-11）\n因此也可以由：\n    （6-2-12）\n求得。根据以上定义，衡量的是数据总变异性（总平方和）中可由模型解释的变异性（回归平方和）所占的比例，其取值在0到1之间，越靠近1，则数据中可由模型解释的成分越多，从而代表模型性能越好；越接近0，则数据中的模型不可解释成分越多，模型性能越不好。\n例6-2-1（b） 线性回归评价举例。\nprint('r_square = ',linreg.score(x,y))\nLinearRegression.score方法能直接返回。我们看到6-2-1（a）中回归模型约0.76，可见并不是特别好。",
        "output": "```json\n[{\"head\": \"线性回归\", \"relation\": \"包含\", \"tail\": \"LinearRegression\"}, {\"head\": \"线性回归\", \"relation\": \"包含\", \"tail\": \"RMSE\"}, {\"head\": \"线性回归\", \"relation\": \"相关\", \"tail\": \"决定系数\"}]\n```"
    },
    {
        "text": "5.1.2特征的数据类型及意义\n数据的每一列（特征）在文件中的存储数据类型是什么？其对应的实际意义是什么？由此实际意义决定的实际数据类型又是什么？\n做数据分析时，我们常常将特征分为以下几种数据类型。\n（1）数值型数据\n数值型数据指具有数量上的意义，支持比较大小，同时还支持加减乘除、求算术平均等基本数学运算的那些数据。数值型数据在计算机中常常体现为整数或浮点数的存储形式。\n（2）排序型数据\n排序型数据也具有数量上的定义，所以支持比较大小，但不一定满足基本的数学运算。例如我们通常的排名数据，我们可以定义第1名最好，排名数字越大对应排名越差，所以排名是可以相互比较大小的。但是排名数据并不支持加法运算，例如第1名+第2名并不等于第3名，第1名与第3名的算术平均也并不一定就等于第2名，所以通常的排名，尽管可能以整数体现，但并不能当成线性域定义的数值来对待。\n排序型数据在计算机中可能出现整数和字符型的存储形式。\n（3）类别型数据\n类别型数据则没有数量上的对应性，因此没有大小的意义，仅仅作为一个标记符号，表示出一个类别与其他类别的不同，或者是一个样本与其他样本的不同。\n类别型数据在计算机中最常见的是字符型的存储形式，但也不乏以整数存储的情况。当出现以整数存储的类别数据时，要特别注意，尽管对其做数学运算不违背语法规则，但却是没有实际意义的。\n（4）逻辑型/布尔型数据\n类别型数据中，如果只存在两种非此即彼的选择，则一般称为逻辑型数据或布尔型数据。\n逻辑性数据在计算机中可以体现为布尔存储形式（True或False），也可能体现为整数0、1的存储形式。逻辑型数据支持的运算与数学运算不同，是我们所说的逻辑运算。\n例题5-1-2 Titanic数据集中的特征检查。\n结合各个特征的实际意义，我们会发现这12个特征呈现几种不同的数据类型。例如：\n特征‘年龄’‘同行平辈人数’‘同行父母或子女人数’‘船费’是有数量上的意义的，不同的值之间能比较大小，如‘人数’4大于‘人数’2，‘年龄’52大于‘年龄’34，也支持基本的数学运算，例如加减法、求算术平均，等等，因此，这些都是我们通常说的数值型的数据。\n特征‘性别’‘姓名’‘票号’‘登船港口’是以字符（串）存储型的类别型数据，不具备数量上的意义，不支持数学运算。\n特征‘是否生还’是什么类型？数据文件中各个样本在该特征上的取值是数字‘0’或‘1’，所以它是数值型数据吗？\n判断一个数据是否是数值型的，关键是考察其是否具备数量的意义（能比较大小），以及是否支持基本的数学运算。特征‘是否生还’有两个取值，1（是）、0（否），‘1’比‘0’大吗？这两种取值做加减法会有意义吗？显然这两个问题的答案都是否定的。所以它不是数值型数据。对于这种只有两个备选，且非此即彼的数据类型，我们通常称为逻辑型，或者布尔型。\n再来看特征‘仓位等级’，它是数值型吗？如果‘仓位等级’在定义的时候已经认定‘1’代表最好的仓位，数字越大仓位等级越低，那么这个特征有数量上的定义，可以比较大小。但是，一个1等舱加上一个2等仓，并不等于1个3等仓，所以它也不是一个数值型数据。我们可以认为这是一种排序类型，是依据某个度量排序得到的，具备一定的量化意义。如果这里的数字‘1’~‘4’仅仅代表了不同的类别，不具备量的意义，即数字大或小都不代表等级高，这时只能认为它属于类别型数据。\n最后，我们看一下特征‘乘客编号’，这个特征也极具迷惑性，好像是数值型或排序型。但是这个编号明显不支持数学运算，所以不是数值型。如果其大小没有特殊的含义，即不是某种度量的排序，那也不是排序型。该特征主要用来区别不同的乘客，所以我们依然可以把它当成类别型来对待。\n之前我们已经介绍过，pandas的read_csv函数将csv文件中的数据导入一个DataFrame结构，DataFrame封装的info函数，能提供一些对数据的基本分析，例如数据的规模、每个特征有多少非空值，以及特征存储时采用的数据类型。要注意的是，这里的存储数据类型并不一定是特征的实际意义上的数据类型，但能给我们提供一定参考。\n可见，对于特征，我们首先可依据‘是否具备量化意义（即是否可比较大小），是否支持数学运算’的准则，区分出数值型数据和非数值型数据。对于数值型数据，我们后续可以用算术平均、标准差等量化统计量来分析；而对于非数值型数据，我们则主要依据它们来进行分组与筛选，整个判断流程可以参照图5-1-1。\n图5-1-1 判断数据类型的流程\n例如，我们对上述加载的Titanic数据，根据‘仓位等级’进行分组，对‘船费’进行算术平均统计，就可以获得每种‘仓位等级’对应的平均‘船费’了。\n例5-1-3 Titanic数据集中不同仓位等级的船费统计。\nimport pandas as pd\nmy_data = pd.read_csv(\"Titanic.csv\")\nprint('Table 1. Mean Fare of Group')\nx=my_data.groupby(['PClass']).mean()\nprint(x ['Fare'])",
        "relations": [
            {
                "head": "数据类型",
                "relation": "包含",
                "tail": "数值型数据"
            },
            {
                "head": "数据类型",
                "relation": "包含",
                "tail": "排序型数据"
            },
            {
                "head": "数据类型",
                "relation": "包含",
                "tail": "类别型数据"
            },
            {
                "head": "数据类型",
                "relation": "包含",
                "tail": "逻辑型/布尔型数据"
            }
        ],
        "entities": [
            "数据类型",
            "数值型数据",
            "排序型数据",
            "类别型数据",
            "逻辑型/布尔型数据"
        ],
        "input": "实体列表为: [\"数据类型\",\"数值型数据\", \"排序型数据\", \"类别型数据\", \"逻辑型/布尔型数据\"] 文本片段为: 5.1.2特征的数据类型及意义\n数据的每一列（特征）在文件中的存储数据类型是什么？其对应的实际意义是什么？由此实际意义决定的实际数据类型又是什么？\n做数据分析时，我们常常将特征分为以下几种数据类型。\n（1）数值型数据\n数值型数据指具有数量上的意义，支持比较大小，同时还支持加减乘除、求算术平均等基本数学运算的那些数据。数值型数据在计算机中常常体现为整数或浮点数的存储形式。\n（2）排序型数据\n排序型数据也具有数量上的定义，所以支持比较大小，但不一定满足基本的数学运算。例如我们通常的排名数据，我们可以定义第1名最好，排名数字越大对应排名越差，所以排名是可以相互比较大小的。但是排名数据并不支持加法运算，例如第1名+第2名并不等于第3名，第1名与第3名的算术平均也并不一定就等于第2名，所以通常的排名，尽管可能以整数体现，但并不能当成线性域定义的数值来对待。\n排序型数据在计算机中可能出现整数和字符型的存储形式。\n（3）类别型数据\n类别型数据则没有数量上的对应性，因此没有大小的意义，仅仅作为一个标记符号，表示出一个类别与其他类别的不同，或者是一个样本与其他样本的不同。\n类别型数据在计算机中最常见的是字符型的存储形式，但也不乏以整数存储的情况。当出现以整数存储的类别数据时，要特别注意，尽管对其做数学运算不违背语法规则，但却是没有实际意义的。\n（4）逻辑型/布尔型数据\n类别型数据中，如果只存在两种非此即彼的选择，则一般称为逻辑型数据或布尔型数据。\n逻辑性数据在计算机中可以体现为布尔存储形式（True或False），也可能体现为整数0、1的存储形式。逻辑型数据支持的运算与数学运算不同，是我们所说的逻辑运算。\n例题5-1-2 Titanic数据集中的特征检查。\n结合各个特征的实际意义，我们会发现这12个特征呈现几种不同的数据类型。例如：\n特征‘年龄’‘同行平辈人数’‘同行父母或子女人数’‘船费’是有数量上的意义的，不同的值之间能比较大小，如‘人数’4大于‘人数’2，‘年龄’52大于‘年龄’34，也支持基本的数学运算，例如加减法、求算术平均，等等，因此，这些都是我们通常说的数值型的数据。\n特征‘性别’‘姓名’‘票号’‘登船港口’是以字符（串）存储型的类别型数据，不具备数量上的意义，不支持数学运算。\n特征‘是否生还’是什么类型？数据文件中各个样本在该特征上的取值是数字‘0’或‘1’，所以它是数值型数据吗？\n判断一个数据是否是数值型的，关键是考察其是否具备数量的意义（能比较大小），以及是否支持基本的数学运算。特征‘是否生还’有两个取值，1（是）、0（否），‘1’比‘0’大吗？这两种取值做加减法会有意义吗？显然这两个问题的答案都是否定的。所以它不是数值型数据。对于这种只有两个备选，且非此即彼的数据类型，我们通常称为逻辑型，或者布尔型。\n再来看特征‘仓位等级’，它是数值型吗？如果‘仓位等级’在定义的时候已经认定‘1’代表最好的仓位，数字越大仓位等级越低，那么这个特征有数量上的定义，可以比较大小。但是，一个1等舱加上一个2等仓，并不等于1个3等仓，所以它也不是一个数值型数据。我们可以认为这是一种排序类型，是依据某个度量排序得到的，具备一定的量化意义。如果这里的数字‘1’~‘4’仅仅代表了不同的类别，不具备量的意义，即数字大或小都不代表等级高，这时只能认为它属于类别型数据。\n最后，我们看一下特征‘乘客编号’，这个特征也极具迷惑性，好像是数值型或排序型。但是这个编号明显不支持数学运算，所以不是数值型。如果其大小没有特殊的含义，即不是某种度量的排序，那也不是排序型。该特征主要用来区别不同的乘客，所以我们依然可以把它当成类别型来对待。\n之前我们已经介绍过，pandas的read_csv函数将csv文件中的数据导入一个DataFrame结构，DataFrame封装的info函数，能提供一些对数据的基本分析，例如数据的规模、每个特征有多少非空值，以及特征存储时采用的数据类型。要注意的是，这里的存储数据类型并不一定是特征的实际意义上的数据类型，但能给我们提供一定参考。\n可见，对于特征，我们首先可依据‘是否具备量化意义（即是否可比较大小），是否支持数学运算’的准则，区分出数值型数据和非数值型数据。对于数值型数据，我们后续可以用算术平均、标准差等量化统计量来分析；而对于非数值型数据，我们则主要依据它们来进行分组与筛选，整个判断流程可以参照图5-1-1。\n图5-1-1 判断数据类型的流程\n例如，我们对上述加载的Titanic数据，根据‘仓位等级’进行分组，对‘船费’进行算术平均统计，就可以获得每种‘仓位等级’对应的平均‘船费’了。\n例5-1-3 Titanic数据集中不同仓位等级的船费统计。\nimport pandas as pd\nmy_data = pd.read_csv(\"Titanic.csv\")\nprint('Table 1. Mean Fare of Group')\nx=my_data.groupby(['PClass']).mean()\nprint(x ['Fare'])",
        "output": "```json\n[{\"head\": \"数据类型\", \"relation\": \"包含\", \"tail\": \"数值型数据\"},{\"head\": \"数据类型\", \"relation\": \"包含\", \"tail\": \"排序型数据\"},{\"head\": \"数据类型\", \"relation\": \"包含\", \"tail\": \"类别型数据\"},{\"head\": \"数据类型\", \"relation\": \"包含\", \"tail\": \"逻辑型/布尔型数据\"}]\n```"
    },
    {
        "text": "7.2.2 展示可视化的三点基本原则\n具体到操作上，我们建议在展示阶段的可视化遵从以下三点基本原则。\n1.可视化的具体方法视我们要传递的信息而定\n我们通常的展示可大致分为对状态（由参数或数据描述）的展示、对过程的展示和对关系的展示（见图7-2-1）。其中对状态的展示又可能涉及时序变化展示、分布展示、（分组）对比展示，等等。\n图7-2-1 根据要传递的信息确定可视化方法（思维导图形式）\n时序变化展示是指展示某个参数随时间的变化。最基本的时序变化展示方式就是绘制时序图，也就是用X轴来代表时间，Y轴代表该量化参数，把参数随时间的演变曲线描记下来，如图7-2-2，就是描记的一位ICU病人连续三天中心率随一天的时间从早到晚变化的时序图。在一些高级可视化中，用一个具体的可变形象代表要表现的参数，然后采用动画的方式来直观表现其随时间的演化，有助于给观众留下深刻印象。\n图7-2-2 时序图举例\n（图中就1位患者不同3天中的心率，展示了其时序变化，横轴代表时间在一天中的不同时刻，纵轴则代表对应时刻测量得到的心率。三条不同颜色不同标记的曲线分别代表不同的三天。）\n图7-2-3 分布展示时的可视化方法选取\n分布展示是指要反映某个参数随其他参数的变化。例如常见的一种是样本发生概率（或样本出现频次）随某参数或参数区间的变化，联系第5章的描述性统计中的知识，直方图（如例5-3-5）是常用的一种展示方式。而如果要展示一个数值型参数随另一个或两个数值型参数的变化，或者同时体现样本在两个或三个数值型参数上的分布，则可以尝试散点图（如例5-3-9）描述，此时点的聚集趋势能反映出参数间的相互关系，而点的疏密程度能定性地反映出样本在不同参数区间中出现的频次。还有一种常见的情况是展示某个参数随空间的变化，例如样本出现频次随空间位置的变化，或其他非频次参数随空间位置的变化。此时，最直接的展示手段就是构造参数的分布地图，在有些领域也被称为地形图、热力图等。如图7-2-4中，将主要城市的位置在地图中用圆标记出来后，用圆内的不同颜色来表示各城市的空气质量指数（air quality index, AQI），色调从冷到暖对应着空气质量从好到坏，因此图中容易看出华北一带城市的空气质量比其他区域普遍要差一些的总体趋势。试想一下，如果不采用这种地图展示，而是直接将300多个城市的AQI用bar图在一张图中画出来，观众想要从中获得一个整体印象则要困难得多。总体而言，当要展示的数据条目非常多，而数据本身又有明确的坐标（位置）信息时，采用地图就是一种非常直观而高效的展示方式。\n图7-2-4 分布地图举例\n（图中用分布地图展示了中国300多个主要城市2015年的空气质量指数，每个小圆圈对应一个城市，其内的颜色则代表空气质量指数）\n对比展示是指要展示的信息不是分立的某个参数，而是参数在不同分组之间的比较结果，强调的是组间的对比关系和差异性（如果有的话）。前述的时序变化展示和分布展示都可以直接拿来进行组间对比，只要其本身能体现出足够醒目的组间差异。更简单地，则是对可比较大小的数值型参数进行组间比较，此时，第5章介绍过的柱状（bar）图、箱型图等都是不错的选择。而当对比的若干组总量恒定时，常见的饼图用来表示各组的占比及其排序关系也是一目了然。\n例7-2-1 用于比较的箱型图绘制举例。\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nmy_data = pd.read_csv(\"C:\\Python\\Scripts\\my_data\\german_credit_data_dataset.csv\")\nmy_dict={'A71':'unemployed', 'A72':'< 1 year', 'A73': '1 - 4 years',\n         'A74' : '4 - 7 years', 'A75' : '>= 7 years'}\nfig=plt.figure(figsize=(10,6))\nsns.set(style='whitegrid')\nsns.set_context(\"talk\")\nh=sns.boxplot(x='present_employment',y='credit_amount',data=my_data,\n           palette=sns.color_palette(\"ch:2.5,-.2,dark=.3\"),\n           linewidth=2,width=0.5,fliersize=6,\n           order=['A75','A74','A73','A72','A71'])\nax=plt.gca()\nax.set_xticklabels([my_dict['A75'],my_dict['A74'],my_dict['A73'],\n                    my_dict['A72'], my_dict['A71']],fontsize=15)\nax.set_xlabel('Employment Status')\nax.set_ylabel('Credit Amount (DM)')\nax.text(2.5,17600,'1000 Applicants in total',color='b')\n例7-2-2 考察1000个贷款申请客户的雇员状态占比并绘制饼图。\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nmy_data = pd.read_csv(\"C:\\Python\\Scripts\\my_data\\german_credit_data_dataset.csv\")\nplt_data=my_data[['duration','present_employment']].groupby('present_employment')\nshare = plt_data.count().values/plt_data.count().values.sum()\nlabels = ['Unemployed', '< 1 year', '1 - 4 years', '4 - 7 years', '>= 7 years']\nexplode = [0, 0, 0, 0, 0.1]\nfig = plt.figure(figsize=(6,5))\nplt.pie(share, explode = explode,\n        labels = labels, autopct = '%3.1f%%',\n        startangle = 180, shadow = True,\n        colors = ['red','tomato', 'yellowgreen', 'springgreen', 'lime'])\nplt.title(\"Employment status of 1000 applicants\")\nplt.show()\n除了对状态的可视化，对过程和关系的可视化展示常常也是需要的，此时不必局限于用代码编程来实现，而是可以借鉴各种方便的绘图工具或应用软件。\n过程的展示一般须包含构成完整过程的关键环节，并在图中明确表明各环节的先后顺序，以及各环节的关键输入与输出等。我们常见的流程图（见图4-5-2）、原理框图、数据流图（见图1-5-1）等，都可归为过程的可视化，在第6章介绍过的决策树的可视化也是一种过程的可视化展示。\n关系的展示则一般包含要研究的所有对象，并在这些对象间明确表示两两间的关系。网络图、层次图、树状图等都可以用来展示关系，如图7-2-5中的网络图，用点与点之间的连线的虚实来表示两点间关系的加强或减弱的变化，而具体的变化程度则用颜色来对应。此外，近年来流行的思维导图也可视为一种关系的可视化展示，如图7-2-1所示。\n图7-2-5 网络图举例\n（图中用网络图展示了不同节点间的某种相关关系在不同状态下的改变。节点间有连线即表示节点间的此种关系有明显改变，实现代表增强型改变，虚线代表减弱型改变，颜色则对应具体改变量的大小。）\n2.可视化要具备自明性，既包含图本身，也包含图中注解\n采用可视化手段后，我们一般不会再用大量的文字对图进行说明，此时若希望观众能迅速抓住图中传递的信息，必须在图中给与必要的注解。\n最基本的要求是，对于图中要表示的各种参数，都要明确标注。例如在二维坐标系中，X轴代表什么、Y轴代表什么（见图7-2-2）；两轴各自的单位（量纲），如果有，是什么；坐标轴上的刻度代表多大的数值范围。再例如，我们在地形图中，用不同颜色渲染所代表的参数是什么，其单位（如果有）是什么，颜色条（colorbar）对应的该参数的取值范围是什么，等等，都需要在图中明确标注（如图7-2-4空气质量地图）。\n然后是对图中分组信息的区分和标注。在进行分组对比时，为不引起混淆，常常用不同的颜色或标记符来区分不同的组，那么各种颜色、标记符各自对应哪个组，必须在图中给出注解，如图7-2-2时序图中用三种不同的颜色和标记符来区分不同的数据采集日期。\n3.合理选择一幅图中的信息容量和信息分辨率\n可视化的目的是快速地给观众留下深刻印象，成功与否还必须考虑人通常的在短时间之内的信息接受容限。一般而言，如果是非逻辑连贯的信息，一幅图中包含超过6个，观众要快速接受就存在困难了。例如，我们要对比用7个不同颜色（或标记符）来表示的7条时序曲线，如果这7条曲线之间并没有一个统一、连贯的关系，那这幅图在一般人看来就很难获取到其中的重点。近年来特别热门的思维导图，其本身作为一种供深入学习的组织与检索图是适宜的，但体系宏大、过于错综复杂的思维导图，作为展示手段，其能快速给观众的印象仅限于“哇哦，这是个复杂的体系”，如果想快速让观众获取其中细节信息，这种思维导图并不合适。同样的原则也适用于网络图。\n另一方面，在同样的区间（范围）内，现在的计算机表达数据的精细程度已远远超过人的视觉分辨率。所以，我们在可视化展示时，必须考虑到人眼本身的分辨力局限，如果想展示的细节超出了人眼分辨力的极限，那么这种展示也达不到预期效果。\n例7-2-3，以下是对ebay上出售的所有注册年份在2016年之前的德国二手车按40个品牌分类的价格信息展示图，你认为这是一种成功的可视化吗？\n解析：总体而言，上图不能算是成功的可视化，因为读者无法从中快速获得信息，甚至图中也根本没有尝试归纳出某种信息。其中未经逻辑组织的数据有40条，我们不仅不能明白这些数据之间的相互关系，即便是它们各自对应哪个品牌这种基本信息，读者也完全无法看清，因为x坐标轴上的品牌标注完全重叠在一起。如果真地希望包含完整的40个品牌的价格信息，分组也许是一种可能的解决方法。\n在改进的图中，我们把与其他品牌差异最大也是价格最高的4个品牌的价格（均值）单独用bar画出来了，其余36个品牌，依然依据价格划分为“高价”“中等”“低价”三组，分别用bar画出三组的平均价格，这样整幅图按价格降序排列提供总共7个数据，读者还是比较容易接受的。我们想突出的重点（4个顶级品牌，以及它们与其他36个品牌之间的巨大的价格差异）相信都会给读者留下印象。而其他36个品牌，我们可以根据图中标注的品牌分组，快速检索到该品牌处于什么价位，以及该品牌处于同等价位中的价格排名。至于smart到底比citroen价格贵出多少，相信在目前的价格区间中，体现真地不那么明显。\n从例7-2-3中我们可以体会到，结果展示时的可视化，首先要有清晰的展示逻辑，然后要让展示方式能凸显你想传递的逻辑信息。同时，尽管大数据时代，无论从数据层面还是工具层面，都可以支持大量信息的可视化，但作为展示手段，并不是信息包含越多就越好。\n本章中，首先介绍了面向不同对象时的结果展示逻辑，了解了面向不同对象时应有不同的侧重，例如面向出资方应侧重介绍项目结果，面向用户应侧重介绍项目成果如何能融入并改善用户的工作，面向数据科学家同行应侧重项目的关键技术、方法及结论。我们还专门就展示过程中的可视化进行了介绍，了解了可视化要兼顾科学与人两个层面，并建议了展示过程中可视化的基本原则，包括：①可视化的具体方法视我们要传递的信息而定；②可视化要具备自明性，既包含图本身，也包含图中注解；③合理选择一幅图中的信息容量和信息分辨率。恰当的展示能给人深刻的正面印象，起到积极的效果，所以其中的各种原则也是不容忽视。",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 7.2.2 展示可视化的三点基本原则\n具体到操作上，我们建议在展示阶段的可视化遵从以下三点基本原则。\n1.可视化的具体方法视我们要传递的信息而定\n我们通常的展示可大致分为对状态（由参数或数据描述）的展示、对过程的展示和对关系的展示（见图7-2-1）。其中对状态的展示又可能涉及时序变化展示、分布展示、（分组）对比展示，等等。\n图7-2-1 根据要传递的信息确定可视化方法（思维导图形式）\n时序变化展示是指展示某个参数随时间的变化。最基本的时序变化展示方式就是绘制时序图，也就是用X轴来代表时间，Y轴代表该量化参数，把参数随时间的演变曲线描记下来，如图7-2-2，就是描记的一位ICU病人连续三天中心率随一天的时间从早到晚变化的时序图。在一些高级可视化中，用一个具体的可变形象代表要表现的参数，然后采用动画的方式来直观表现其随时间的演化，有助于给观众留下深刻印象。\n图7-2-2 时序图举例\n（图中就1位患者不同3天中的心率，展示了其时序变化，横轴代表时间在一天中的不同时刻，纵轴则代表对应时刻测量得到的心率。三条不同颜色不同标记的曲线分别代表不同的三天。）\n图7-2-3 分布展示时的可视化方法选取\n分布展示是指要反映某个参数随其他参数的变化。例如常见的一种是样本发生概率（或样本出现频次）随某参数或参数区间的变化，联系第5章的描述性统计中的知识，直方图（如例5-3-5）是常用的一种展示方式。而如果要展示一个数值型参数随另一个或两个数值型参数的变化，或者同时体现样本在两个或三个数值型参数上的分布，则可以尝试散点图（如例5-3-9）描述，此时点的聚集趋势能反映出参数间的相互关系，而点的疏密程度能定性地反映出样本在不同参数区间中出现的频次。还有一种常见的情况是展示某个参数随空间的变化，例如样本出现频次随空间位置的变化，或其他非频次参数随空间位置的变化。此时，最直接的展示手段就是构造参数的分布地图，在有些领域也被称为地形图、热力图等。如图7-2-4中，将主要城市的位置在地图中用圆标记出来后，用圆内的不同颜色来表示各城市的空气质量指数（air quality index, AQI），色调从冷到暖对应着空气质量从好到坏，因此图中容易看出华北一带城市的空气质量比其他区域普遍要差一些的总体趋势。试想一下，如果不采用这种地图展示，而是直接将300多个城市的AQI用bar图在一张图中画出来，观众想要从中获得一个整体印象则要困难得多。总体而言，当要展示的数据条目非常多，而数据本身又有明确的坐标（位置）信息时，采用地图就是一种非常直观而高效的展示方式。\n图7-2-4 分布地图举例\n（图中用分布地图展示了中国300多个主要城市2015年的空气质量指数，每个小圆圈对应一个城市，其内的颜色则代表空气质量指数）\n对比展示是指要展示的信息不是分立的某个参数，而是参数在不同分组之间的比较结果，强调的是组间的对比关系和差异性（如果有的话）。前述的时序变化展示和分布展示都可以直接拿来进行组间对比，只要其本身能体现出足够醒目的组间差异。更简单地，则是对可比较大小的数值型参数进行组间比较，此时，第5章介绍过的柱状（bar）图、箱型图等都是不错的选择。而当对比的若干组总量恒定时，常见的饼图用来表示各组的占比及其排序关系也是一目了然。\n例7-2-1 用于比较的箱型图绘制举例。\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nmy_data = pd.read_csv(\"C:\\Python\\Scripts\\my_data\\german_credit_data_dataset.csv\")\nmy_dict={'A71':'unemployed', 'A72':'< 1 year', 'A73': '1 - 4 years',\n         'A74' : '4 - 7 years', 'A75' : '>= 7 years'}\nfig=plt.figure(figsize=(10,6))\nsns.set(style='whitegrid')\nsns.set_context(\"talk\")\nh=sns.boxplot(x='present_employment',y='credit_amount',data=my_data,\n           palette=sns.color_palette(\"ch:2.5,-.2,dark=.3\"),\n           linewidth=2,width=0.5,fliersize=6,\n           order=['A75','A74','A73','A72','A71'])\nax=plt.gca()\nax.set_xticklabels([my_dict['A75'],my_dict['A74'],my_dict['A73'],\n                    my_dict['A72'], my_dict['A71']],fontsize=15)\nax.set_xlabel('Employment Status')\nax.set_ylabel('Credit Amount (DM)')\nax.text(2.5,17600,'1000 Applicants in total',color='b')\n例7-2-2 考察1000个贷款申请客户的雇员状态占比并绘制饼图。\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nmy_data = pd.read_csv(\"C:\\Python\\Scripts\\my_data\\german_credit_data_dataset.csv\")\nplt_data=my_data[['duration','present_employment']].groupby('present_employment')\nshare = plt_data.count().values/plt_data.count().values.sum()\nlabels = ['Unemployed', '< 1 year', '1 - 4 years', '4 - 7 years', '>= 7 years']\nexplode = [0, 0, 0, 0, 0.1]\nfig = plt.figure(figsize=(6,5))\nplt.pie(share, explode = explode,\n        labels = labels, autopct = '%3.1f%%',\n        startangle = 180, shadow = True,\n        colors = ['red','tomato', 'yellowgreen', 'springgreen', 'lime'])\nplt.title(\"Employment status of 1000 applicants\")\nplt.show()\n除了对状态的可视化，对过程和关系的可视化展示常常也是需要的，此时不必局限于用代码编程来实现，而是可以借鉴各种方便的绘图工具或应用软件。\n过程的展示一般须包含构成完整过程的关键环节，并在图中明确表明各环节的先后顺序，以及各环节的关键输入与输出等。我们常见的流程图（见图4-5-2）、原理框图、数据流图（见图1-5-1）等，都可归为过程的可视化，在第6章介绍过的决策树的可视化也是一种过程的可视化展示。\n关系的展示则一般包含要研究的所有对象，并在这些对象间明确表示两两间的关系。网络图、层次图、树状图等都可以用来展示关系，如图7-2-5中的网络图，用点与点之间的连线的虚实来表示两点间关系的加强或减弱的变化，而具体的变化程度则用颜色来对应。此外，近年来流行的思维导图也可视为一种关系的可视化展示，如图7-2-1所示。\n图7-2-5 网络图举例\n（图中用网络图展示了不同节点间的某种相关关系在不同状态下的改变。节点间有连线即表示节点间的此种关系有明显改变，实现代表增强型改变，虚线代表减弱型改变，颜色则对应具体改变量的大小。）\n2.可视化要具备自明性，既包含图本身，也包含图中注解\n采用可视化手段后，我们一般不会再用大量的文字对图进行说明，此时若希望观众能迅速抓住图中传递的信息，必须在图中给与必要的注解。\n最基本的要求是，对于图中要表示的各种参数，都要明确标注。例如在二维坐标系中，X轴代表什么、Y轴代表什么（见图7-2-2）；两轴各自的单位（量纲），如果有，是什么；坐标轴上的刻度代表多大的数值范围。再例如，我们在地形图中，用不同颜色渲染所代表的参数是什么，其单位（如果有）是什么，颜色条（colorbar）对应的该参数的取值范围是什么，等等，都需要在图中明确标注（如图7-2-4空气质量地图）。\n然后是对图中分组信息的区分和标注。在进行分组对比时，为不引起混淆，常常用不同的颜色或标记符来区分不同的组，那么各种颜色、标记符各自对应哪个组，必须在图中给出注解，如图7-2-2时序图中用三种不同的颜色和标记符来区分不同的数据采集日期。\n3.合理选择一幅图中的信息容量和信息分辨率\n可视化的目的是快速地给观众留下深刻印象，成功与否还必须考虑人通常的在短时间之内的信息接受容限。一般而言，如果是非逻辑连贯的信息，一幅图中包含超过6个，观众要快速接受就存在困难了。例如，我们要对比用7个不同颜色（或标记符）来表示的7条时序曲线，如果这7条曲线之间并没有一个统一、连贯的关系，那这幅图在一般人看来就很难获取到其中的重点。近年来特别热门的思维导图，其本身作为一种供深入学习的组织与检索图是适宜的，但体系宏大、过于错综复杂的思维导图，作为展示手段，其能快速给观众的印象仅限于“哇哦，这是个复杂的体系”，如果想快速让观众获取其中细节信息，这种思维导图并不合适。同样的原则也适用于网络图。\n另一方面，在同样的区间（范围）内，现在的计算机表达数据的精细程度已远远超过人的视觉分辨率。所以，我们在可视化展示时，必须考虑到人眼本身的分辨力局限，如果想展示的细节超出了人眼分辨力的极限，那么这种展示也达不到预期效果。\n例7-2-3，以下是对ebay上出售的所有注册年份在2016年之前的德国二手车按40个品牌分类的价格信息展示图，你认为这是一种成功的可视化吗？\n解析：总体而言，上图不能算是成功的可视化，因为读者无法从中快速获得信息，甚至图中也根本没有尝试归纳出某种信息。其中未经逻辑组织的数据有40条，我们不仅不能明白这些数据之间的相互关系，即便是它们各自对应哪个品牌这种基本信息，读者也完全无法看清，因为x坐标轴上的品牌标注完全重叠在一起。如果真地希望包含完整的40个品牌的价格信息，分组也许是一种可能的解决方法。\n在改进的图中，我们把与其他品牌差异最大也是价格最高的4个品牌的价格（均值）单独用bar画出来了，其余36个品牌，依然依据价格划分为“高价”“中等”“低价”三组，分别用bar画出三组的平均价格，这样整幅图按价格降序排列提供总共7个数据，读者还是比较容易接受的。我们想突出的重点（4个顶级品牌，以及它们与其他36个品牌之间的巨大的价格差异）相信都会给读者留下印象。而其他36个品牌，我们可以根据图中标注的品牌分组，快速检索到该品牌处于什么价位，以及该品牌处于同等价位中的价格排名。至于smart到底比citroen价格贵出多少，相信在目前的价格区间中，体现真地不那么明显。\n从例7-2-3中我们可以体会到，结果展示时的可视化，首先要有清晰的展示逻辑，然后要让展示方式能凸显你想传递的逻辑信息。同时，尽管大数据时代，无论从数据层面还是工具层面，都可以支持大量信息的可视化，但作为展示手段，并不是信息包含越多就越好。\n本章中，首先介绍了面向不同对象时的结果展示逻辑，了解了面向不同对象时应有不同的侧重，例如面向出资方应侧重介绍项目结果，面向用户应侧重介绍项目成果如何能融入并改善用户的工作，面向数据科学家同行应侧重项目的关键技术、方法及结论。我们还专门就展示过程中的可视化进行了介绍，了解了可视化要兼顾科学与人两个层面，并建议了展示过程中可视化的基本原则，包括：①可视化的具体方法视我们要传递的信息而定；②可视化要具备自明性，既包含图本身，也包含图中注解；③合理选择一幅图中的信息容量和信息分辨率。恰当的展示能给人深刻的正面印象，起到积极的效果，所以其中的各种原则也是不容忽视。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "4.4.6 Pandas. DataFrame\nPandas包中的DataFrame不要求元素的数据类型一致，而且完全可以等同于一个包含行索引和列标题的二维表格。\n例4-4-9（a） Pandas.DataFrame的创建举例。\nimport pandas as pd\nimport numpy as np\nmy_dataframe=pd.DataFrame(np.random.randn(4,5),\n                          index=['a','b','c','d'],\n                          columns=['A','B','C','D','E'])\nmy_dataframe\n例4-4-9（a）中，我们用Pandas的DataFrame命令创建了一个名为my_dataframe的数据框结构，内容是一个4行5列的表格，其行索引由参数index指定，是小写字母a~d，其列标题则由参数columns指定，是大写字母A~E，表格中4行5列的数据则用随机数来填充。\n例4-4-9（b）Pandas.DataFrame访问举例。\nmy_dataframe[['B','C']]   #指定列访问1，返回有列标题，返回值仍是dataframe\nmy_dataframe[['B']]  #指定列访问2，返回有列标题，返回值仍是dataframe\nmy_dataframe['B']  #指定列访问3，返回无列标题，返回值是序列series\nmy_dataframe.iloc[1] #指定行访问1，指定行号\nmy_dataframe.loc['b']  #指定行访问2，指定行索引\n对DataFrame的访问很有意思，指定行和指定列访问是不一样的。如例4-4-9（b）中所示，通过“数据框名[[列标题]]”的格式，可以获取指定列构成的子表（包含行序号和列标题），而“数据框名[列标题]”则获取指定列的内容，返回一个序列（含行序号，但无列标题）。而要访问指定行，我们则需要借助DataFrame内嵌的iloc方法或loc方法，其中iloc方法中指定的是行序号（数），而loc方法中指定的是行索引名（字符）。 \n总结一下，本节主要介绍了6个主要的数据结构，列表、元组、字典、集合、numpy的多维数组ndarray，以及pandas的数据框DataFrame，这些数据结构就是我们后续数据生成或导入、处理和分析的直接对象。",
        "relations": [],
        "entities": [
            "DataFrame"
        ],
        "input": "实体列表为: [\"DataFrame\"] 文本片段为: 4.4.6 Pandas. DataFrame\nPandas包中的DataFrame不要求元素的数据类型一致，而且完全可以等同于一个包含行索引和列标题的二维表格。\n例4-4-9（a） Pandas.DataFrame的创建举例。\nimport pandas as pd\nimport numpy as np\nmy_dataframe=pd.DataFrame(np.random.randn(4,5),\n                          index=['a','b','c','d'],\n                          columns=['A','B','C','D','E'])\nmy_dataframe\n例4-4-9（a）中，我们用Pandas的DataFrame命令创建了一个名为my_dataframe的数据框结构，内容是一个4行5列的表格，其行索引由参数index指定，是小写字母a~d，其列标题则由参数columns指定，是大写字母A~E，表格中4行5列的数据则用随机数来填充。\n例4-4-9（b）Pandas.DataFrame访问举例。\nmy_dataframe[['B','C']]   #指定列访问1，返回有列标题，返回值仍是dataframe\nmy_dataframe[['B']]  #指定列访问2，返回有列标题，返回值仍是dataframe\nmy_dataframe['B']  #指定列访问3，返回无列标题，返回值是序列series\nmy_dataframe.iloc[1] #指定行访问1，指定行号\nmy_dataframe.loc['b']  #指定行访问2，指定行索引\n对DataFrame的访问很有意思，指定行和指定列访问是不一样的。如例4-4-9（b）中所示，通过“数据框名[[列标题]]”的格式，可以获取指定列构成的子表（包含行序号和列标题），而“数据框名[列标题]”则获取指定列的内容，返回一个序列（含行序号，但无列标题）。而要访问指定行，我们则需要借助DataFrame内嵌的iloc方法或loc方法，其中iloc方法中指定的是行序号（数），而loc方法中指定的是行索引名（字符）。 \n总结一下，本节主要介绍了6个主要的数据结构，列表、元组、字典、集合、numpy的多维数组ndarray，以及pandas的数据框DataFrame，这些数据结构就是我们后续数据生成或导入、处理和分析的直接对象。",
        "output": "```json\n[]\n```"
    },
    {
        "text": "1.2.2大数据\n数据量多大规模时能称之为大数据呢？其实，目前对于“大数据”并没有明确的规模界定。有观点认为，当数据量大到需要用并行计算工具处理时，即是大数据。但当我们定义大数据时，却常常不只看数据体量这一个维度。公认地，大数据具有通常所说的4V特点：\n(1)体量大（high volume）。\n(2)产生速度快而时效性高（high velocity）。数据增长速度快，同时数据具有高度的时效性，随着时间改变，数据的价值会很快流失，因而对数据处理速度的要求也很高，例如很多情况下都要求能进行实时分析。\n（3）类型繁多（high variety）。不仅仅是单一类型的数据，文本、语音、图像等不同媒介的数据，结构化、非结构化的数据全部混杂在一起。\n（4）高度真实而价值密度低（high veracity）。数据价值与数据体量之间没有正比例关系，大体量的数据中常常只蕴藏了很小的价值，例如24小时监控数据中有用的可能只有其中极少的部分，高度真实和冗余伴随的是低价值密度。\n在上述4V特点下，相应地大数据的分析处理也较传统数据分析有所改变。例如有部分观点认为大数据使我们面临数据的全体，而不再是部分或抽样；又如有人认为在大数据时代我们对不精确性的容忍度变得更高了，可接受一定的混杂性；再如大数据分析更着重刻画或寻找相关性，而非因果性。这些观点在应用层面有一定的适用性，但就科学层面而言，却并未获得学术届的一致认可。例如，我们获得的数据量比以往大了，但并不一定就真的是“全体”了，特别是，当前绝大多数的数据科学任务其根本都是从已知推测未知，既然存在“未知”，那么已知的就不是“全体”，所以这里所谓的“全体”，只是一个相对的概念；再如，现阶段我们可以只关注相关性，但如果可以做到，我们并不排斥考察因果性，或者说科学总是力图诠释因果的；最后，对于不精确性的容忍，其本质上是用数据的多样化和大体量带来的冗余来补偿的，当这种补偿达不到预期效果时，对数据精确性的要求也势必会提高。\n总之，随着“大数据”时代的深入，我们对于海量数据分析的理解也在不断尝试中调整，真正具备科学意义的本质概念与原则会在这一过程中沉淀下来。\n例1-2-1 相关性和因果性举例\n关于关注“相关”而非“因果”，我们不妨通过一个小故事来体会一下。1993年，美国学者艾格拉沃提出了Aprior算法，可以通过分析购物篮中的商品集合，找出商品间关系，然后研究或推测顾客的购买行为。沃尔玛公司很快就将Aprior算法引入了他们的POS机数据分析。此后，超市管理人员分析销售数据时发现了一个有趣的现象：有一些看似毫无联系的商品，例如“啤酒”和“尿布”，会经常出现在同一个购物篮中。于是，沃尔玛在布置卖场时就将这些体现出“相关性”的商品摆放在一起，并获得了商品销售收入的提升。在这里，是“啤酒”销售导致了“尿布”销售，亦或是反过来，都不是关注的重点，即不强调“因果性”。但既然两者有“相关性”，那就摆放在一起方便顾客拿取，以达到促进销售的目的。",
        "relations": [
            {
                "head": "大数据",
                "relation": "包含",
                "tail": "大数据的4V特点"
            }
        ],
        "entities": [
            "大数据",
            "大数据的4V特点"
        ],
        "input": "实体列表为: [\"大数据\", \"大数据的4V特点\"] 文本片段为: 1.2.2大数据\n数据量多大规模时能称之为大数据呢？其实，目前对于“大数据”并没有明确的规模界定。有观点认为，当数据量大到需要用并行计算工具处理时，即是大数据。但当我们定义大数据时，却常常不只看数据体量这一个维度。公认地，大数据具有通常所说的4V特点：\n(1)体量大（high volume）。\n(2)产生速度快而时效性高（high velocity）。数据增长速度快，同时数据具有高度的时效性，随着时间改变，数据的价值会很快流失，因而对数据处理速度的要求也很高，例如很多情况下都要求能进行实时分析。\n（3）类型繁多（high variety）。不仅仅是单一类型的数据，文本、语音、图像等不同媒介的数据，结构化、非结构化的数据全部混杂在一起。\n（4）高度真实而价值密度低（high veracity）。数据价值与数据体量之间没有正比例关系，大体量的数据中常常只蕴藏了很小的价值，例如24小时监控数据中有用的可能只有其中极少的部分，高度真实和冗余伴随的是低价值密度。\n在上述4V特点下，相应地大数据的分析处理也较传统数据分析有所改变。例如有部分观点认为大数据使我们面临数据的全体，而不再是部分或抽样；又如有人认为在大数据时代我们对不精确性的容忍度变得更高了，可接受一定的混杂性；再如大数据分析更着重刻画或寻找相关性，而非因果性。这些观点在应用层面有一定的适用性，但就科学层面而言，却并未获得学术届的一致认可。例如，我们获得的数据量比以往大了，但并不一定就真的是“全体”了，特别是，当前绝大多数的数据科学任务其根本都是从已知推测未知，既然存在“未知”，那么已知的就不是“全体”，所以这里所谓的“全体”，只是一个相对的概念；再如，现阶段我们可以只关注相关性，但如果可以做到，我们并不排斥考察因果性，或者说科学总是力图诠释因果的；最后，对于不精确性的容忍，其本质上是用数据的多样化和大体量带来的冗余来补偿的，当这种补偿达不到预期效果时，对数据精确性的要求也势必会提高。\n总之，随着“大数据”时代的深入，我们对于海量数据分析的理解也在不断尝试中调整，真正具备科学意义的本质概念与原则会在这一过程中沉淀下来。\n例1-2-1 相关性和因果性举例\n关于关注“相关”而非“因果”，我们不妨通过一个小故事来体会一下。1993年，美国学者艾格拉沃提出了Aprior算法，可以通过分析购物篮中的商品集合，找出商品间关系，然后研究或推测顾客的购买行为。沃尔玛公司很快就将Aprior算法引入了他们的POS机数据分析。此后，超市管理人员分析销售数据时发现了一个有趣的现象：有一些看似毫无联系的商品，例如“啤酒”和“尿布”，会经常出现在同一个购物篮中。于是，沃尔玛在布置卖场时就将这些体现出“相关性”的商品摆放在一起，并获得了商品销售收入的提升。在这里，是“啤酒”销售导致了“尿布”销售，亦或是反过来，都不是关注的重点，即不强调“因果性”。但既然两者有“相关性”，那就摆放在一起方便顾客拿取，以达到促进销售的目的。",
        "output": "```json\n[{\"head\": \"大数据\", \"relation\": \"包含\", \"tail\": \"大数据的4V特点\"}]\n```"
    },
    {
        "text": "1. 工具包的离线安装\n离线安装时，需要先将安装文件下载到本机。例如，在图4-2-3的页面上单击“Download files”链接，进入到文件列表（见图4-2-4），我们看到，包文件的后缀名是“whl”，俗称轮子文件。列表中包含了各种版本、适用于不同操作系统的包文件，选择你需要的版本下载存盘。安装时，在“命令提示符”程序中，切换工作目录到Python安装目录下的Scripts目录，然后在命令行输入“pip install” 及完整的工具包安装文件名，按回车键，即可进行安装了。\n图4-2-3 PYPI上的NumPy包页面截图\n图4-2-4 PYPI上的NumPy包下载文件列表页面截图",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 1. 工具包的离线安装\n离线安装时，需要先将安装文件下载到本机。例如，在图4-2-3的页面上单击“Download files”链接，进入到文件列表（见图4-2-4），我们看到，包文件的后缀名是“whl”，俗称轮子文件。列表中包含了各种版本、适用于不同操作系统的包文件，选择你需要的版本下载存盘。安装时，在“命令提示符”程序中，切换工作目录到Python安装目录下的Scripts目录，然后在命令行输入“pip install” 及完整的工具包安装文件名，按回车键，即可进行安装了。\n图4-2-3 PYPI上的NumPy包页面截图\n图4-2-4 PYPI上的NumPy包下载文件列表页面截图",
        "output": "```json\n[]\n```"
    },
    {
        "text": "思考题\n就大数据时代数据分析的三大变革（“部分数据”到“全体数据”，“因果性”到“相关性”，“精确性”到“混杂性”），请谈谈你的体会与看法。\n对于150个鸢尾花数据（50个setosa、50个versicolor、50个virginica）实施“是setosa”和“不是setosa”的二分类，能实现总体判别accuracy为65%，请思考这个分类模型可接受吗？",
        "relations": [],
        "entities": [],
        "input": "实体列表为: [] 文本片段为: 思考题\n就大数据时代数据分析的三大变革（“部分数据”到“全体数据”，“因果性”到“相关性”，“精确性”到“混杂性”），请谈谈你的体会与看法。\n对于150个鸢尾花数据（50个setosa、50个versicolor、50个virginica）实施“是setosa”和“不是setosa”的二分类，能实现总体判别accuracy为65%，请思考这个分类模型可接受吗？",
        "output": "```json\n[]\n```"
    },
    {
        "text": "6.1.4 p-hacking\n假设检验返回的p值是假设检验的重要结果之一，它反映的是基于当前空假设H0，现有样本计算出的检验统计量落入当前值及以外区域的概率，p值越小，表示这个概率越小，小到一定程度，例如小于我们设置的显著性水平，则可以拒绝H0。但是，需要提醒的是，此时我们拒绝空假设依然存在着错误拒绝的风险，错误拒绝的概率就是p。本质上，p反映的就是I型错误率，而反映的则是我们能接受的I型错误率的上限。\n再换一个角度，现在有一些数据分析工作，会尝试在大量特征中寻找（例如）的特征。这样找到的特征真地能说明统计学差异吗？事实上，即便H0成立，当我们随机尝试100种不同的特征时，出现5个落入拒绝域（）的特征也是完全合理的，因为这里0.05的意义本来就是H0为真时落入到拒绝域的概率。因此，我们一定要警惕这种过度挖掘数据的做法，这其实就是近些年引起关注和诟病的p-hacking（即p值操纵）。\n为避免p-hacking，传统统计学强调应先提假设，再做假设检验。例如，当想要看某个特征在两组之间有没有差别时，应先提H0：特征X在组A和组B上均值相同（或分布相同等），然后在做好混杂因素匹配的情况下进行数据准备和假设检验。而不是无目标地在大量特征中穷尽搜索，例如没有任何假设，把所有的特征甚至构造复合特征，逐个在组A、组B间检验后，挑出其中p值较小的来声称这些特征存在组间显著性差异。\n那么，是不是就不能考察大量特征了呢？也不尽然。我们认为最好的做法是用可重复性来验证，就是说，当分析一批数据找到p值小的特征后，先不要急于下结论，应针对该特征确定空假设，再充分搜集独立的新数据，并检查在新数据上这些专门针对该空假设的假设检验是否依然具有小的p值。在新数据上能重复的结论，才是可靠的。我们不要忘记，可重复性始终是区分科学与伪科学的一条重要原则。",
        "relations": [
            {
                "head": "假设检验",
                "relation": "包含",
                "tail": "p值"
            },
            {
                "head": "p值",
                "relation": "相关",
                "tail": "p-hacking"
            }
        ],
        "entities": [
            "假设检验",
            "p值",
            "p-hacking"
        ],
        "input": "实体列表为: [\"假设检验\", \"p值\", \"p-hacking\"] 文本片段为: 6.1.4 p-hacking\n假设检验返回的p值是假设检验的重要结果之一，它反映的是基于当前空假设H0，现有样本计算出的检验统计量落入当前值及以外区域的概率，p值越小，表示这个概率越小，小到一定程度，例如小于我们设置的显著性水平，则可以拒绝H0。但是，需要提醒的是，此时我们拒绝空假设依然存在着错误拒绝的风险，错误拒绝的概率就是p。本质上，p反映的就是I型错误率，而反映的则是我们能接受的I型错误率的上限。\n再换一个角度，现在有一些数据分析工作，会尝试在大量特征中寻找（例如）的特征。这样找到的特征真地能说明统计学差异吗？事实上，即便H0成立，当我们随机尝试100种不同的特征时，出现5个落入拒绝域（）的特征也是完全合理的，因为这里0.05的意义本来就是H0为真时落入到拒绝域的概率。因此，我们一定要警惕这种过度挖掘数据的做法，这其实就是近些年引起关注和诟病的p-hacking（即p值操纵）。\n为避免p-hacking，传统统计学强调应先提假设，再做假设检验。例如，当想要看某个特征在两组之间有没有差别时，应先提H0：特征X在组A和组B上均值相同（或分布相同等），然后在做好混杂因素匹配的情况下进行数据准备和假设检验。而不是无目标地在大量特征中穷尽搜索，例如没有任何假设，把所有的特征甚至构造复合特征，逐个在组A、组B间检验后，挑出其中p值较小的来声称这些特征存在组间显著性差异。\n那么，是不是就不能考察大量特征了呢？也不尽然。我们认为最好的做法是用可重复性来验证，就是说，当分析一批数据找到p值小的特征后，先不要急于下结论，应针对该特征确定空假设，再充分搜集独立的新数据，并检查在新数据上这些专门针对该空假设的假设检验是否依然具有小的p值。在新数据上能重复的结论，才是可靠的。我们不要忘记，可重复性始终是区分科学与伪科学的一条重要原则。",
        "output": "```json\n[{\"head\": \"假设检验\", \"relation\": \"包含\", \"tail\": \"p值\"}, {\"head\": \"p值\", \"relation\": \"相关\", \"tail\": \"p-hacking\"}]\n```"
    },
    {
        "text": "3.1.4 确定数据构成\n通过了可行性分析之后，就可以确定数据的构成了。通常而言，我们后续方便处理的数据都是“结构化”的数据。结构化的数据可以理解为一张不能再细分的二维表，表中一行代表一个存在且唯一的样本，一列代表一个属性。例如，例3-1-2的方案A中，一架飞机就是一个样本，飞机各部位的弹孔密度、飞机总体残损程度则是相应的属性。对一架具体的飞机，其对应的总体残损程度和各部位弹孔密度分别填入一行中的各列，不同的飞机填入不同的行，如表3-1-1所示，这样就构造出飞机数据的二维表格，也就是结构化的数据了。\n表3-1-1 例3-1-3中方案A的结构化数据\n我们再看一下降低银行不良贷款率任务的例子。第2章中，我们已明确这是一个对申请贷款的用户做出“普通客户”/“高风险客户”二分类的任务。接下来，你会提出怎样的前提假设？如果你认为放贷风险与申请客户的历史信用、贷款目的、贷款金额、还款能力等有关系，那你能确定你需要哪些数据吗？对于这些数据，你有切实可行的获取方法吗？确定好可行的数据方案后，我们构造一张申请贷款的客户资料二维表格（见表3-1-2），表格中每一行代表一位客户，每一列则反映一个我们想要获取的客户数据，例如历史信用、贷款期数（月）、贷款目的（购车、购房、教育还是普通消费）、贷款金额、储蓄或理财账户的余额、可支配月收入与月还款比、房产、担保、抚养赡养人数、职业、婚姻状况、已有贷款金额和月供，等等，这样，贷款甄别任务的数据构成就确定了。\n表3-1-2 贷款客户甄别任务中的结构化数据\n总结一下，在明确任务之后，我们需要提出前提假设。不同的假设很可能会涉及不同的数据，最终导致不同的研究内容，因此，在真正进入数据收集环节之前，应认真审视，确定与事实最符合的假设。然后，根据前提假设，给出对应的数据方案，并认真思考获取方案中数据的可行性，如果方案中的数据是在项目期限内无法获得的，就必须及时做出调整。最后，根据确定下来的数据方案，就可以确定出数据构成了。",
        "relations": [],
        "entities": [
            "结构化数据"
        ],
        "input": "实体列表为: [\"结构化数据\"] 文本片段为: 3.1.4 确定数据构成\n通过了可行性分析之后，就可以确定数据的构成了。通常而言，我们后续方便处理的数据都是“结构化”的数据。结构化的数据可以理解为一张不能再细分的二维表，表中一行代表一个存在且唯一的样本，一列代表一个属性。例如，例3-1-2的方案A中，一架飞机就是一个样本，飞机各部位的弹孔密度、飞机总体残损程度则是相应的属性。对一架具体的飞机，其对应的总体残损程度和各部位弹孔密度分别填入一行中的各列，不同的飞机填入不同的行，如表3-1-1所示，这样就构造出飞机数据的二维表格，也就是结构化的数据了。\n表3-1-1 例3-1-3中方案A的结构化数据\n我们再看一下降低银行不良贷款率任务的例子。第2章中，我们已明确这是一个对申请贷款的用户做出“普通客户”/“高风险客户”二分类的任务。接下来，你会提出怎样的前提假设？如果你认为放贷风险与申请客户的历史信用、贷款目的、贷款金额、还款能力等有关系，那你能确定你需要哪些数据吗？对于这些数据，你有切实可行的获取方法吗？确定好可行的数据方案后，我们构造一张申请贷款的客户资料二维表格（见表3-1-2），表格中每一行代表一位客户，每一列则反映一个我们想要获取的客户数据，例如历史信用、贷款期数（月）、贷款目的（购车、购房、教育还是普通消费）、贷款金额、储蓄或理财账户的余额、可支配月收入与月还款比、房产、担保、抚养赡养人数、职业、婚姻状况、已有贷款金额和月供，等等，这样，贷款甄别任务的数据构成就确定了。\n表3-1-2 贷款客户甄别任务中的结构化数据\n总结一下，在明确任务之后，我们需要提出前提假设。不同的假设很可能会涉及不同的数据，最终导致不同的研究内容，因此，在真正进入数据收集环节之前，应认真审视，确定与事实最符合的假设。然后，根据前提假设，给出对应的数据方案，并认真思考获取方案中数据的可行性，如果方案中的数据是在项目期限内无法获得的，就必须及时做出调整。最后，根据确定下来的数据方案，就可以确定出数据构成了。",
        "output": "```json\n[]\n```"
    }
]